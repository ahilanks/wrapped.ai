{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a728510",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import os\n",
    "# import tensorflow as tf\n",
    "# import tensorflow_datasets as tfds\n",
    "# from tensorboard.plugins import projector\n",
    "import supabase\n",
    "from supabase import create_client, Client\n",
    "\n",
    "SUPABASE_URL = \"https://aqavgmrcggugruedqtzv.supabase.co\"\n",
    "SUPABASE_KEY = \"eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJzdXBhYmFzZSIsInJlZiI6ImFxYXZnbXJjZ2d1Z3J1ZWRxdHp2Iiwicm9sZSI6ImFub24iLCJpYXQiOjE3NTA1NDA5MTMsImV4cCI6MjA2NjExNjkxM30.f4RSnwdPVkSpApBUuzZlYnG63Y-3SUQtYkAhXpi3tFk\"\n",
    "from supabase import create_client                    \n",
    "client = create_client(SUPABASE_URL, SUPABASE_KEY)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f3d92f9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>conversation_id</th>\n",
       "      <th>email</th>\n",
       "      <th>title</th>\n",
       "      <th>body</th>\n",
       "      <th>embeddings_json</th>\n",
       "      <th>created_at</th>\n",
       "      <th>company</th>\n",
       "      <th>author_role</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6855f553-dfb0-800d-8522-15d97f41534f</td>\n",
       "      <td>hello.devpatel@gmail.com</td>\n",
       "      <td>Model Benchmark Folder Issue</td>\n",
       "      <td>import onnx\\r\\nimport os\\r\\nimport onnx.helper...</td>\n",
       "      <td>None</td>\n",
       "      <td>20259 days, 23:57:07.682000</td>\n",
       "      <td>gpt</td>\n",
       "      <td>user</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6855f553-dfb0-800d-8522-15d97f41534f</td>\n",
       "      <td>hello.devpatel@gmail.com</td>\n",
       "      <td>Model Benchmark Folder Issue</td>\n",
       "      <td>**Why one model folder ends up empty**\\n\\nThe ...</td>\n",
       "      <td>None</td>\n",
       "      <td>20259 days, 23:57:08.157641</td>\n",
       "      <td>gpt</td>\n",
       "      <td>assistant</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>6855f553-dfb0-800d-8522-15d97f41534f</td>\n",
       "      <td>hello.devpatel@gmail.com</td>\n",
       "      <td>Model Benchmark Folder Issue</td>\n",
       "      <td>for context, these inference json files that a...</td>\n",
       "      <td>None</td>\n",
       "      <td>20260 days, 0:05:00.190000</td>\n",
       "      <td>gpt</td>\n",
       "      <td>user</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6855f553-dfb0-800d-8522-15d97f41534f</td>\n",
       "      <td>hello.devpatel@gmail.com</td>\n",
       "      <td>Model Benchmark Folder Issue</td>\n",
       "      <td>Right – the JSONs you listed (`compile_summary...</td>\n",
       "      <td>None</td>\n",
       "      <td>20260 days, 0:05:00.541043</td>\n",
       "      <td>gpt</td>\n",
       "      <td>assistant</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6855f1ea-cb14-800d-9a0f-7b43adc0b297</td>\n",
       "      <td>hello.devpatel@gmail.com</td>\n",
       "      <td>Profile Artifact Issue</td>\n",
       "      <td>import onnx\\r\\nimport os\\r\\nimport onnx.helper...</td>\n",
       "      <td>None</td>\n",
       "      <td>20259 days, 23:42:34.551000</td>\n",
       "      <td>gpt</td>\n",
       "      <td>user</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        conversation_id                     email  \\\n",
       "0  6855f553-dfb0-800d-8522-15d97f41534f  hello.devpatel@gmail.com   \n",
       "1  6855f553-dfb0-800d-8522-15d97f41534f  hello.devpatel@gmail.com   \n",
       "2  6855f553-dfb0-800d-8522-15d97f41534f  hello.devpatel@gmail.com   \n",
       "3  6855f553-dfb0-800d-8522-15d97f41534f  hello.devpatel@gmail.com   \n",
       "4  6855f1ea-cb14-800d-9a0f-7b43adc0b297  hello.devpatel@gmail.com   \n",
       "\n",
       "                          title  \\\n",
       "0  Model Benchmark Folder Issue   \n",
       "1  Model Benchmark Folder Issue   \n",
       "2  Model Benchmark Folder Issue   \n",
       "3  Model Benchmark Folder Issue   \n",
       "4        Profile Artifact Issue   \n",
       "\n",
       "                                                body embeddings_json  \\\n",
       "0  import onnx\\r\\nimport os\\r\\nimport onnx.helper...            None   \n",
       "1  **Why one model folder ends up empty**\\n\\nThe ...            None   \n",
       "2  for context, these inference json files that a...            None   \n",
       "3  Right – the JSONs you listed (`compile_summary...            None   \n",
       "4  import onnx\\r\\nimport os\\r\\nimport onnx.helper...            None   \n",
       "\n",
       "                    created_at company author_role  \n",
       "0  20259 days, 23:57:07.682000     gpt        user  \n",
       "1  20259 days, 23:57:08.157641     gpt   assistant  \n",
       "2   20260 days, 0:05:00.190000     gpt        user  \n",
       "3   20260 days, 0:05:00.541043     gpt   assistant  \n",
       "4  20259 days, 23:42:34.551000     gpt        user  "
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "from typing import List, Dict, Any\n",
    "import pandas as pd\n",
    "import numpy as np                    # ← was missing\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "def load_conversations(path: str | Path) -> List[Dict[str, Any]]:\n",
    "    path = Path(path)\n",
    "    with path.open(\"r\", encoding=\"utf-8\") as f:\n",
    "        conversations: List[Dict[str, Any]] = json.load(f)\n",
    "    if not isinstance(conversations, list):\n",
    "        raise ValueError(\n",
    "            f\"Expected a list of conversations, got {type(conversations).__name__}\"\n",
    "        )\n",
    "    return conversations\n",
    "\n",
    "\n",
    "def _to_text(part: Any) -> str:\n",
    "    if isinstance(part, str):\n",
    "        return part\n",
    "    if isinstance(part, dict) and \"text\" in part:\n",
    "        return part[\"text\"]\n",
    "    return json.dumps(part, ensure_ascii=False)\n",
    "def parse_chatgpt_json(conv: Dict[str, Any]) -> pd.DataFrame:\n",
    "    rows = []\n",
    "    conv_id = conv.get(\"conversation_id\")\n",
    "    title   = conv.get(\"title\", \"\")\n",
    "\n",
    "    for node in conv.get(\"mapping\", {}).values():\n",
    "        msg = node.get(\"message\")\n",
    "        if msg is None:\n",
    "            continue\n",
    "\n",
    "        parts = msg.get(\"content\", {}).get(\"parts\", [])\n",
    "        body  = \"\\n\".join(_to_text(p) for p in parts).strip()\n",
    "        if not body:\n",
    "            continue\n",
    "\n",
    "        rows.append(\n",
    "            {\n",
    "                \"conversation_id\": conv_id,\n",
    "                \"email\": \"hello.devpatel@gmail.com\",\n",
    "                \"title\": title,\n",
    "                \"body\": body,\n",
    "                \"embeddings_json\": None,\n",
    "                \"created_at\": datetime.utcfromtimestamp(msg[\"create_time\"]).isoformat(), \n",
    "                \"company\": \"gpt\",\n",
    "                \"author_role\": (msg.get(\"author\") or {}).get(\"role\"),\n",
    "            }\n",
    "        )\n",
    "\n",
    "    col_order = [\n",
    "        \"conversation_id\", \"email\", \"title\", \"body\",\n",
    "        \"embeddings_json\",          \n",
    "        \"created_at\", \"company\", \"author_role\",\n",
    "    ]\n",
    "    return pd.DataFrame(rows)[col_order]\n",
    "\n",
    "def conversations_to_dataframe(\n",
    "    path: str | Path,\n",
    "    drop_empty: bool = True,\n",
    "    drop_empty_convs: bool = True,\n",
    ") -> pd.DataFrame:\n",
    "    \n",
    "    convs = load_conversations(path)\n",
    "    dfs   = [parse_chatgpt_json(c) for c in convs]\n",
    "    df    = pd.concat(dfs, ignore_index=True)\n",
    "\n",
    "    if drop_empty:\n",
    "        df = df[df[\"body\"].str.strip().astype(bool)]\n",
    "\n",
    "    if drop_empty_convs:\n",
    "        df = df[df[\"conversation_id\"].isin(df[\"conversation_id\"].unique())]\n",
    "\n",
    "    # replace ±inf with NaN, then convert NaN → None\n",
    "    df = df.replace([np.inf, -np.inf], np.nan)\n",
    "    clean_df = df.where(pd.notnull(df), None)\n",
    "\n",
    "    return clean_df.reset_index(drop=True)\n",
    "\n",
    "\n",
    "DATA_PATH = \"data/conversations.json\"\n",
    "full_df = conversations_to_dataframe(DATA_PATH)\n",
    "full_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8d99b23",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from __future__ import annotations\n",
    "\n",
    "import re\n",
    "import collections\n",
    "from datetime import datetime, timedelta\n",
    "from dataclasses import dataclass\n",
    "from typing import Dict, List, Any, Iterable\n",
    "\n",
    "import pandas as pd\n",
    "import networkx as nx\n",
    "import spacy\n",
    "from llama_index.core import (\n",
    "    Document, Settings, VectorStoreIndex, StorageContext,\n",
    ")\n",
    "from llama_index.core.schema import BaseNode, TextNode\n",
    "from llama_index.core.node_parser import SentenceSplitter\n",
    "from llama_index.embeddings.voyageai import VoyageEmbedding\n",
    "from llama_index.vector_stores.supabase import SupabaseVectorStore\n",
    "\n",
    "# ── Config (move secrets to env vars in real code!) ──────────────────────────\n",
    "SUPABASE_URL  = \"https://aqavgmrcggugruedqtzv.supabase.co\"\n",
    "SERVICE_ROLE  = (\n",
    "    \"eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJzdXBhYmFzZSIsInJlZiI6ImFxYXZnbXJjZ2d1Z3J1ZWRxdHp2Iiwicm9sZSI6InNlcnZpY2Vfcm9sZSIsImlhdCI6MTc1MDU0MDkxMywiZXhwIjoyMDY2MTE2OTEzfQ.m18otDH2Bm2z__QX02n1iUGGSp4pGMkRrquMnPywPwM\"\n",
    ")\n",
    "POSTGRES_CONN = \"postgresql://postgres:Wrapped12345!@db.aqavgmrcggugruedqtzv.supabase.co:5432/postgres\"\n",
    "VOYAGE_KEY    = \"pa-z3YYZAFZnRW0fte9GEpN2dYHGC4dB8H5CMAFeg3lIue\"\n",
    "MODEL_NAME    = \"voyage-3-lite\"      # 512‑D\n",
    "DIMENSION     = 512\n",
    "\n",
    "CONV_TBL = CHUNK_TBL = SENT_TBL = \"chat_logs_final\"\n",
    "anthropic_client = Anthropic()\n",
    "\n",
    "# Get API keys from environment\n",
    "ANTHROPIC_API_KEY = os.getenv(\"ANTHROPIC_API_KEY\", \"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\")\n",
    "\n",
    "def name_cluster(examples: list[str]) -> str:\n",
    "    prompt = (\n",
    "      \"You’re a helpful assistant. \"\n",
    "      \"Given these snippets from one cluster, suggest a short topic label:\\n\\n\"\n",
    "      + \"\\n\".join(f\"- {e}\" for e in examples[:10])\n",
    "      + \"\\n\\nLabel:\"\n",
    "    )\n",
    "    resp = anthropic_client.messages.create(\n",
    "        model=\"claude-3-opus-20240229\",\n",
    "        max_tokens=10,\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}]\n",
    "    )\n",
    "    return resp.content[0].text.strip()\n",
    "\n",
    "\n",
    "# ── Helper dataclass ─────────────────────────────────────────────────────────\n",
    "@dataclass\n",
    "class EmbedConfig:\n",
    "    conv_chunk: int = 8192\n",
    "    chunk: int      = 512\n",
    "    sent: int       = 256\n",
    "    chunk_ov: int   = 100\n",
    "    sent_ov: int    = 20\n",
    "\n",
    "\n",
    "# ── MiniChatEmbedder ─────────────────────────────────────────────────────────\n",
    "class MiniChatEmbedder:\n",
    "    _DAYS_RE = re.compile(r\"(\\d+)\\s+days,\\s+([\\d:.]+)\")\n",
    "\n",
    "    def __init__(self, cfg: EmbedConfig | None = None) -> None:\n",
    "        self.cfg = cfg or EmbedConfig()\n",
    "\n",
    "        # Initialise spaCy (small model is fine ‑ speed > recall here)\n",
    "        self.nlp = spacy.load(\"en_core_web_sm\", disable=[\"parser\", \"lemmatizer\"])\n",
    "        self.nlp.add_pipe(\"sentencizer\")  # explicit sentence boundaries\n",
    "\n",
    "        # One dummy init (required by SupabaseVectorStore)\n",
    "        self._bootstrap_vector_collection()\n",
    "\n",
    "        # Embedding model (Voyage)\n",
    "        Settings.embed_model   = VoyageEmbedding(\n",
    "            model_name       = MODEL_NAME,\n",
    "            voyage_api_key   = VOYAGE_KEY,\n",
    "            input_type       = \"document\",\n",
    "        )\n",
    "        Settings.chunk_size    = self.cfg.chunk\n",
    "        Settings.chunk_overlap = self.cfg.chunk_ov\n",
    "\n",
    "        # pgvector stores – reuse same table, different columns\n",
    "        self.stores = {\n",
    "            \"conversation\": SupabaseVectorStore(\n",
    "                postgres_connection_string = POSTGRES_CONN,\n",
    "                collection_name           = CONV_TBL,\n",
    "                dimension                 = DIMENSION,\n",
    "                overwrite_collection      = False,\n",
    "                vector_column             = \"embedding_conv_512\",\n",
    "            ),\n",
    "            \"chunk\": SupabaseVectorStore(\n",
    "                postgres_connection_string = POSTGRES_CONN,\n",
    "                collection_name           = CHUNK_TBL,\n",
    "                dimension                 = DIMENSION,\n",
    "                overwrite_collection      = False,\n",
    "                vector_column             = \"embedding_chunk_512\",\n",
    "            ),\n",
    "            \"sentence\": SupabaseVectorStore(\n",
    "                postgres_connection_string = POSTGRES_CONN,\n",
    "                collection_name           = SENT_TBL,\n",
    "                dimension                 = DIMENSION,\n",
    "                overwrite_collection      = False,\n",
    "                vector_column             = \"embedding_sentence_512\",\n",
    "            ),\n",
    "        }\n",
    "\n",
    "        # Splitters\n",
    "        self.parsers = {\n",
    "            \"conversation\": SentenceSplitter(\n",
    "                chunk_size=self.cfg.conv_chunk, chunk_overlap=200\n",
    "            ),\n",
    "            \"chunk\": SentenceSplitter(\n",
    "                chunk_size=self.cfg.chunk, chunk_overlap=self.cfg.chunk_ov\n",
    "            ),\n",
    "            \"sentence\": SentenceSplitter(\n",
    "                chunk_size=self.cfg.sent, chunk_overlap=self.cfg.sent_ov\n",
    "            ),\n",
    "        }\n",
    "\n",
    "        # entity‑co‑occurrence graph (NetworkX)\n",
    "        self.G: nx.Graph = nx.Graph()\n",
    "\n",
    "    # ────── Public API ────────────────────────────────────────────────────\n",
    "    def ingest(self, df: pd.DataFrame) -> Dict[str, VectorStoreIndex]:\n",
    "        \"\"\"Embed data‑frame & update entity graph.\"\"\"\n",
    "        df      = self._sanitize(df)\n",
    "        convs   = self._df_to_conversations(df)\n",
    "        nodes   = self._conversations_to_nodes(convs)        # + entity extraction inside\n",
    "\n",
    "        # insert nodes into vector DBs\n",
    "        indices: Dict[str, VectorStoreIndex] = {}\n",
    "        for lvl, lvl_nodes in nodes.items():\n",
    "            if not lvl_nodes:\n",
    "                continue\n",
    "            ctx = StorageContext.from_defaults(vector_store=self.stores[lvl])\n",
    "            idx = VectorStoreIndex(lvl_nodes, storage_context=ctx, show_progress=True)\n",
    "            indices[lvl] = idx\n",
    "        return indices\n",
    "\n",
    "    def search(self, query: str, level: str = \"chunk\", k: int = 5) -> List[Dict[str, Any]]:\n",
    "        if level not in self.stores:\n",
    "            raise ValueError(f\"level must be one of {list(self.stores)}\")\n",
    "        ctx  = StorageContext.from_defaults(vector_store=self.stores[level])\n",
    "        idx  = VectorStoreIndex([], storage_context=ctx)\n",
    "        hits = idx.as_retriever(similarity_top_k=k).retrieve(query)\n",
    "        return [{\"text\": h.text, \"score\": h.score, \"metadata\": h.metadata} for h in hits]\n",
    "\n",
    "    def spotify_wrapped(self, df: pd.DataFrame, user_id: str | None = None) -> Dict[str, Any]:\n",
    "        df = self._sanitize(df)\n",
    "\n",
    "        num_chats    = df.conversation_id.nunique()\n",
    "        num_messages = len(df)\n",
    "\n",
    "        user_df      = df[df.author_role.eq(\"user\")]\n",
    "        assistant_df = df[df.author_role.eq(\"assistant\")]\n",
    "\n",
    "        ent_counter: collections.Counter[str] = collections.Counter()\n",
    "        for doc in self.nlp.pipe(user_df.body.tolist(), batch_size=128):\n",
    "            for ent in doc.ents:\n",
    "                txt = ent.text.strip()\n",
    "                if len(txt) < 3:                # skip 1–2 char tokens\n",
    "                    continue\n",
    "                if txt.isdigit():\n",
    "                    continue\n",
    "                if re.fullmatch(r\"[#\\\\d/_.]+\", txt):\n",
    "                    continue\n",
    "                ent_counter[txt] += 1\n",
    "\n",
    "        top_entities = ent_counter.most_common(15)\n",
    "\n",
    "        queries = (\n",
    "            user_df.body\n",
    "            .map(lambda t: \" \".join(t.strip().split()[:8]).lower())\n",
    "            .value_counts()\n",
    "            .head(10)\n",
    "            .to_dict()\n",
    "        )\n",
    "\n",
    "        response_tokens = int(assistant_df.body.str.split().map(len).sum())\n",
    "\n",
    "        langs = [\n",
    "            \"python\",\"javascript\",\"typescript\",\"c++\",\"c\",\"java\",\"go\",\"rust\",\"ruby\",\"php\",\"scala\",\"sql\",\n",
    "        ]\n",
    "        lang_counts = {\n",
    "            lang: int(df.body.str.contains(fr\"\\b{re.escape(lang)}\\b\", case=False).sum())\n",
    "            for lang in langs\n",
    "        }\n",
    "        most_common_languages = {k: v for k, v in sorted(lang_counts.items(), key=lambda p: p[1], reverse=True) if v}\n",
    "\n",
    "        most_active_hour = int(df.created_at.dt.hour.value_counts().idxmax()) if not df.empty else None\n",
    "\n",
    "        wrapped = {\n",
    "            \"year\":          datetime.utcnow().year,\n",
    "            \"user_id\":       user_id or \"default\",\n",
    "            \"num_chats\":     num_chats,\n",
    "            \"num_messages\":  num_messages,\n",
    "            \"response_tokens\": response_tokens,\n",
    "            \"top_entities\":  top_entities,         # (entity, freq)\n",
    "            \"top_queries\":   queries,\n",
    "            \"languages\":     most_common_languages,\n",
    "            \"most_active_hour\": most_active_hour,\n",
    "            \"generated_at\":  datetime.utcnow().isoformat(),\n",
    "        }\n",
    "        return wrapped\n",
    "\n",
    "\n",
    "    def graph_summary(self, top_k: int = 10) -> Dict[str, Any]:\n",
    "        if self.G.number_of_nodes() == 0:\n",
    "            return {}\n",
    "        pr = nx.pagerank(self.G, weight=\"weight\")\n",
    "        top_nodes = sorted(pr.items(), key=lambda x: x[1], reverse=True)[:top_k]\n",
    "        bridge_edges = sorted(\n",
    "            self.G.edges(data=True), key=lambda e: e[2][\"weight\"], reverse=True\n",
    "        )[: top_k]\n",
    "        return {\n",
    "            \"pagerank_top\": top_nodes,\n",
    "            \"bridge_edges\": [(s, t, d[\"weight\"]) for s, t, d in bridge_edges],\n",
    "        }\n",
    "\n",
    "    @staticmethod\n",
    "    def _bootstrap_vector_collection(self, top_k: int = 10) -> Dict[str, Any]:\n",
    "        if self.G.number_of_nodes() == 0:\n",
    "            return {}\n",
    "        pr = nx.pagerank(self.G, weight=\"weight\")\n",
    "        top_nodes = sorted(pr.items(), key=lambda x: x[1], reverse=True)[:top_k]\n",
    "        # Bridge edges = edges with high betweenness centrality approximated by weight\n",
    "        bridge_edges = sorted(self.G.edges(data=True), key=lambda e: e[2][\"weight\"], reverse=True)[: top_k]\n",
    "        return {\n",
    "            \"pagerank_top\": top_nodes,\n",
    "            \"bridge_edges\": [(s, t, d[\"weight\"]) for s, t, d in bridge_edges],\n",
    "        }\n",
    "\n",
    "    # ────── Internal helpers ────────────────────────────────────────────\n",
    "    @staticmethod\n",
    "    def _bootstrap_vector_collection():\n",
    "        # Dummy instantiation so SupabaseVectorStore auto‑creates the collection\n",
    "        SupabaseVectorStore(\n",
    "            postgres_connection_string = POSTGRES_CONN,\n",
    "            collection_name            = CONV_TBL,\n",
    "            dimension                  = DIMENSION,\n",
    "            overwrite_collection       = False,\n",
    "        )\n",
    "\n",
    "    def _sanitize(self, df: pd.DataFrame) -> pd.DataFrame:\n",
    "        rename_map = {\"create_time\": \"created_at\", \"timestamp\": \"created_at\"}\n",
    "        df = df.rename(columns={k: v for k, v in rename_map.items() if k in df})\n",
    "\n",
    "        must_have = {\"conversation_id\", \"author_role\", \"body\", \"created_at\"}\n",
    "        missing   = must_have - set(df.columns)\n",
    "        if missing:\n",
    "            raise ValueError(f\"missing columns: {missing}\")\n",
    "\n",
    "        df[\"created_at\"] = df[\"created_at\"].apply(self._parse_created_at)\n",
    "        df = df.dropna(subset=[\"created_at\"])\n",
    "        return df\n",
    "\n",
    "    def _parse_created_at(self, val):\n",
    "        if pd.isna(val):\n",
    "            return pd.NaT\n",
    "        ts = pd.to_datetime(val, errors=\"coerce\", utc=True)\n",
    "        if ts is not pd.NaT:\n",
    "            return ts\n",
    "        m = self._DAYS_RE.fullmatch(str(val).strip())\n",
    "        if m:\n",
    "            days, timestr = int(m.group(1)), m.group(2)\n",
    "            base = datetime(1970, 1, 1, tzinfo=pd.Timestamp.utcnow().tz)\n",
    "            return base + timedelta(days=days) + pd.to_timedelta(timestr)\n",
    "        return pd.NaT\n",
    "\n",
    "    def _df_to_conversations(self, df: pd.DataFrame) -> List[Dict[str, Any]]:\n",
    "        grouped = collections.defaultdict(list)\n",
    "        for r in df.itertuples(index=False):\n",
    "            grouped[r.conversation_id].append(\n",
    "                dict(timestamp=r.created_at, role=r.author_role, content=r.body)\n",
    "            )\n",
    "        convs = []\n",
    "        for cid, msgs in grouped.items():\n",
    "            msgs.sort(key=lambda m: m[\"timestamp\"])\n",
    "            text = \"\\n\\n\".join(f\"[{m['role'].upper()}] {m['content']}\" for m in msgs)\n",
    "            convs.append({\"id\": cid, \"text\": text, \"meta\": {\"conversation_id\": cid}})\n",
    "        return convs\n",
    "\n",
    "    def _conversations_to_nodes(self, convs: List[Dict[str, Any]]) -> Dict[str, List[BaseNode]]:\n",
    "        out: Dict[str, List[BaseNode]] = {lvl: [] for lvl in self.parsers}\n",
    "\n",
    "        for conv in convs:\n",
    "            self.build_entity_graph([conv[\"text\"]])\n",
    "\n",
    "            conv_node = TextNode(\n",
    "                text     = conv[\"text\"],\n",
    "                metadata = {\"level\": \"conversation\", **conv[\"meta\"]},\n",
    "            )\n",
    "            out[\"conversation\"].append(conv_node)\n",
    "\n",
    "            for lvl in (\"chunk\", \"sentence\"):\n",
    "                nodes = self.parsers[lvl].get_nodes_from_documents(\n",
    "                    [Document(text=conv[\"text\"], metadata=conv[\"meta\"])]\n",
    "                )\n",
    "                for i, n in enumerate(nodes):\n",
    "                    # attach entity list for RAG filtering\n",
    "                    ents = [ent.text for ent in self.nlp(n.text).ents if ent.label_ not in {\"DATE\", \"TIME\"}]\n",
    "                    n.metadata.update(\n",
    "                        level              = lvl,\n",
    "                        hierarchy_id       = f\"conv_{conv['id']}_{lvl}_{i}\",\n",
    "                        parent_conversation= conv[\"id\"],\n",
    "                        entities           = ents,\n",
    "                    )\n",
    "                out[lvl].extend(nodes)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d08d9d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedder  = MiniChatEmbedder()\n",
    "indices   = embedder.ingest(full_df)\n",
    "\n",
    "# Initialize embedding model\n",
    "embed_model = VoyageEmbedding(\n",
    "    model_name=\"voyage-3-lite\", \n",
    "    voyage_api_key=VOYAGE_KEY\n",
    ")\n",
    "\n",
    "chunk_splitter    = SentenceSplitter(chunk_size=512, chunk_overlap=100)\n",
    "sentence_splitter = SentenceSplitter(chunk_size=256, chunk_overlap=20)\n",
    "\n",
    "def make_embeddings_json(text: str) -> dict:\n",
    "    conv_emb     = embed_model.get_text_embedding(text)\n",
    "\n",
    "    chunks       = chunk_splitter.split_text(text)\n",
    "    chunk_embeds = embed_model.get_text_embedding_batch(chunks) if chunks else []\n",
    "\n",
    "    sents        = sentence_splitter.split_text(text)\n",
    "    sent_embeds  = embed_model.get_text_embedding_batch(sents)  if sents  else []\n",
    "\n",
    "    return {\n",
    "        \"conversation\": conv_emb,\n",
    "        \"chunks\":       chunk_embeds,\n",
    "        \"sentences\":    sent_embeds,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3c766e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from anthropic import Anthropic\n",
    "import os\n",
    "from typing import List, Dict, Any\n",
    "\n",
    "from llama_index.core import (\n",
    "    VectorStoreIndex,\n",
    "    StorageContext,\n",
    ")\n",
    "from llama_index.core.schema import NodeWithScore\n",
    "\n",
    "\n",
    "class ChatGraphBackend:\n",
    "    \"\"\"Simple wrapper: vector search + direct Anthropic API calls.\"\"\"\n",
    "\n",
    "    def __init__(self, embedder, level: str = \"chunk\") -> None:\n",
    "        if level not in embedder.stores:\n",
    "            raise ValueError(f\"level must be one of {list(embedder.stores)}\")\n",
    "\n",
    "        # Vector index for retrieval\n",
    "        storage_ctx = StorageContext.from_defaults(\n",
    "            vector_store=embedder.stores[level]\n",
    "        )\n",
    "        self.vector_idx = VectorStoreIndex([], storage_context=storage_ctx)\n",
    "        \n",
    "        # Store the entity graph for additional context\n",
    "        self.entity_graph = embedder.G\n",
    "        \n",
    "        # Direct Anthropic client\n",
    "        self.anthropic_client = Anthropic(api_key=ANTHROPIC_API_KEY)\n",
    "\n",
    "    def retrieve(self, query: str, k: int = 8) -> List[NodeWithScore]:\n",
    "        \"\"\"Return *k* context nodes for the question.\"\"\"\n",
    "        retriever = self.vector_idx.as_retriever(similarity_top_k=k)\n",
    "        return retriever.retrieve(query)\n",
    "\n",
    "    def answer(self, query: str, k: int = 8) -> str:\n",
    "        \"\"\"Retrieve context → call Anthropic API → return answer text.\"\"\"\n",
    "        nodes = self.retrieve(query, k=k)\n",
    "        context = \"\\n\\n\".join(n.node.text for n in nodes)\n",
    "\n",
    "        system_prompt = (\n",
    "            \"You are an expert assistant who answers strictly using the \"\n",
    "            \"provided context. If the answer isn't present, say you don't know.\"\n",
    "        )\n",
    "        \n",
    "        prompt = f\"Context:\\n---\\n{context}\\n---\\n\\nQuestion: {query}\\nAnswer:\"\n",
    "        \n",
    "        response = self.anthropic_client.messages.create(\n",
    "            model=\"claude-3-haiku-20240307\",\n",
    "            max_tokens=300,\n",
    "            system=system_prompt,\n",
    "            messages=[{\"role\": \"user\", \"content\": prompt}]\n",
    "        )\n",
    "        \n",
    "        return response.content[0].text.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9db5d9eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "backend = ChatGraphBackend(embedder, level=\"conversation\")\n",
    "answer = backend.answer(\"What are the key topics across each of my conversations? DO not say i dont have enough context take waht u know\")\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e427158f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "def batched_insert(\n",
    "    table_name : str,\n",
    "    rows       : list[dict],\n",
    "    batch_size : int = 500,\n",
    "    max_retries: int = 3,\n",
    "):\n",
    "    tbl = client.table(table_name)\n",
    "\n",
    "    for start in range(0, len(rows), batch_size):\n",
    "        chunk   = rows[start : start + batch_size]\n",
    "\n",
    "        for r in chunk:\n",
    "            if r[\"embeddings_json\"] is None:\n",
    "                conv_emb = embed_model.get_text_embedding(r[\"body\"])\n",
    "                r[\"embeddings_json\"] = json.dumps({\"conversation\": conv_emb})\n",
    "\n",
    "        attempt = 0\n",
    "        while True:\n",
    "            try:\n",
    "                resp = tbl.upsert(chunk).execute()     \n",
    "                print(f\"✓ inserted {len(chunk):4} rows (offset {start})\")\n",
    "                break\n",
    "            except Exception as err:\n",
    "                attempt += 1\n",
    "                if attempt > max_retries:\n",
    "                    raise\n",
    "                wait = 2 ** attempt\n",
    "                print(f\"⚠️  batch {start}-{start+len(chunk)-1} failed ({err}); \"\n",
    "                      f\"retry {attempt} in {wait}s\")\n",
    "                time.sleep(wait)\n",
    "\n",
    "wrapped = embedder.spotify_wrapped(full_df, user_id=\"hello.devpatel@gmail.com\")\n",
    "graph   = embedder.graph_summary()\n",
    "\n",
    "records = full_df.to_dict(orient=\"records\")          \n",
    "for r in records:                                    \n",
    "    r[\"wrapped_json\"] = json.dumps(wrapped)          \n",
    "    r[\"graph_json\"]   = json.dumps(graph)            \n",
    "\n",
    "\n",
    "batched_insert(\"chat_logs_final\", records, batch_size=400)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b9fff91",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "calhacks",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
