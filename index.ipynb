{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bacbc74f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import time\n",
    "import collections\n",
    "import re\n",
    "from pathlib import Path\n",
    "from typing import List, Dict, Any\n",
    "from datetime import datetime, timedelta\n",
    "from dataclasses import dataclass\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "import spacy\n",
    "from supabase import create_client\n",
    "from anthropic import Anthropic\n",
    "\n",
    "from llama_index.core import (\n",
    "    Document, Settings, VectorStoreIndex, StorageContext,\n",
    ")\n",
    "from llama_index.core.schema import BaseNode, TextNode, NodeWithScore\n",
    "from llama_index.core.node_parser import SentenceSplitter\n",
    "from llama_index.embeddings.voyageai import VoyageEmbedding\n",
    "from llama_index.vector_stores.supabase import SupabaseVectorStore\n",
    "\n",
    "# ── Configuration ──────────────────────────────────────────────────\n",
    "SUPABASE_URL = \"https://aqavgmrcggugruedqtzv.supabase.co\"\n",
    "SUPABASE_KEY = \"eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJzdXBhYmFzZSIsInJlZiI6ImFxYXZnbXJjZ2d1Z3J1ZWRxdHp2Iiwicm9sZSI6ImFub24iLCJpYXQiOjE3NTA1NDA5MTMsImV4cCI6MjA2NjExNjkxM30.f4RSnwdPVkSpApBUuzZlYnG63Y-3SUQtYkAhXpi3tFk\"\n",
    "POSTGRES_CONN = \"postgresql://postgres:Wrapped12345!@db.aqavgmrcggugruedqtzv.supabase.co:5432/postgres\"\n",
    "VOYAGE_KEY = \"pa-z3YYZAFZnRW0fte9GEpN2dYHGC4dB8H5CMAFeg3lIue\"\n",
    "ANTHROPIC_API_KEY = \"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\"\n",
    "\n",
    "MODEL_NAME = \"voyage-3-lite\"\n",
    "DIMENSION = 512\n",
    "TABLE_NAME = \"chat_logs_final\"\n",
    "\n",
    "# Initialize clients\n",
    "client = create_client(SUPABASE_URL, SUPABASE_KEY)\n",
    "anthropic_client = Anthropic(api_key=ANTHROPIC_API_KEY)\n",
    "\n",
    "@dataclass\n",
    "class EmbedConfig:\n",
    "    conv_chunk: int = 8192\n",
    "    chunk: int = 512\n",
    "    sent: int = 256\n",
    "    chunk_ov: int = 100\n",
    "    sent_ov: int = 20\n",
    "\n",
    "class MiniChatEmbedder:\n",
    "    _DAYS_RE = re.compile(r\"(\\d+)\\s+days,\\s+([\\d:.]+)\")\n",
    "\n",
    "    def __init__(self, cfg: EmbedConfig = None):\n",
    "        self.cfg = cfg or EmbedConfig()\n",
    "        \n",
    "        # Initialize spaCy\n",
    "        self.nlp = spacy.load(\"en_core_web_sm\", disable=[\"parser\", \"lemmatizer\"])\n",
    "        self.nlp.add_pipe(\"sentencizer\")\n",
    "        \n",
    "        # Initialize embedding model\n",
    "        Settings.embed_model = VoyageEmbedding(\n",
    "            model_name=MODEL_NAME,\n",
    "            voyage_api_key=VOYAGE_KEY,\n",
    "            input_type=\"document\",\n",
    "        )\n",
    "        \n",
    "        # Bootstrap vector collection\n",
    "        self._bootstrap_vector_collection()\n",
    "        \n",
    "        # Initialize vector store for conversations only\n",
    "        self.store = SupabaseVectorStore(\n",
    "            postgres_connection_string=POSTGRES_CONN,\n",
    "            collection_name=TABLE_NAME,\n",
    "            dimension=DIMENSION,\n",
    "            overwrite_collection=False,\n",
    "            vector_column=\"embedding_conv_512\",\n",
    "        )\n",
    "        \n",
    "        # Initialize parser for conversations\n",
    "        self.parser = SentenceSplitter(\n",
    "            chunk_size=self.cfg.conv_chunk, \n",
    "            chunk_overlap=200\n",
    "        )\n",
    "        \n",
    "        # Entity graph\n",
    "        self.G = nx.Graph()\n",
    "\n",
    "    @staticmethod\n",
    "    def _bootstrap_vector_collection():\n",
    "        \"\"\"Create the vector collection if it doesn't exist\"\"\"\n",
    "        SupabaseVectorStore(\n",
    "            postgres_connection_string=POSTGRES_CONN,\n",
    "            collection_name=TABLE_NAME,\n",
    "            dimension=DIMENSION,\n",
    "            overwrite_collection=False,\n",
    "        )\n",
    "\n",
    "    def _sanitize(self, df: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"Clean and validate dataframe\"\"\"\n",
    "        rename_map = {\"create_time\": \"created_at\", \"timestamp\": \"created_at\"}\n",
    "        df = df.rename(columns={k: v for k, v in rename_map.items() if k in df})\n",
    "\n",
    "        must_have = {\"conversation_id\", \"author_role\", \"body\", \"created_at\"}\n",
    "        missing = must_have - set(df.columns)\n",
    "        if missing:\n",
    "            raise ValueError(f\"Missing columns: {missing}\")\n",
    "\n",
    "        df[\"created_at\"] = df[\"created_at\"].apply(self._parse_created_at)\n",
    "        df = df.dropna(subset=[\"created_at\"])\n",
    "        return df\n",
    "\n",
    "    def _parse_created_at(self, val):\n",
    "        \"\"\"Parse various timestamp formats\"\"\"\n",
    "        if pd.isna(val):\n",
    "            return pd.NaT\n",
    "        \n",
    "        # Try standard datetime parsing first\n",
    "        ts = pd.to_datetime(val, errors=\"coerce\", utc=True)\n",
    "        if ts is not pd.NaT:\n",
    "            return ts\n",
    "        \n",
    "        # Try custom format\n",
    "        m = self._DAYS_RE.fullmatch(str(val).strip())\n",
    "        if m:\n",
    "            days, timestr = int(m.group(1)), m.group(2)\n",
    "            base = datetime(1970, 1, 1, tzinfo=pd.Timestamp.utcnow().tz)\n",
    "            return base + timedelta(days=days) + pd.to_timedelta(timestr)\n",
    "        \n",
    "        return pd.NaT\n",
    "\n",
    "    def _df_to_conversations(self, df: pd.DataFrame) -> List[Dict[str, Any]]:\n",
    "        \"\"\"Convert dataframe to conversation format\"\"\"\n",
    "        grouped = collections.defaultdict(list)\n",
    "        for r in df.itertuples(index=False):\n",
    "            grouped[r.conversation_id].append({\n",
    "                \"timestamp\": r.created_at,\n",
    "                \"role\": r.author_role,\n",
    "                \"content\": r.body\n",
    "            })\n",
    "        \n",
    "        convs = []\n",
    "        for cid, msgs in grouped.items():\n",
    "            msgs.sort(key=lambda m: m[\"timestamp\"])\n",
    "            text = \"\\n\\n\".join(f\"[{m['role'].upper()}] {m['content']}\" for m in msgs)\n",
    "            convs.append({\n",
    "                \"id\": cid,\n",
    "                \"text\": text,\n",
    "                \"meta\": {\"conversation_id\": cid}\n",
    "            })\n",
    "        return convs\n",
    "\n",
    "    def build_entity_graph(self, texts: List[str]):\n",
    "        \"\"\"Build entity co-occurrence graph\"\"\"\n",
    "        for text in texts:\n",
    "            doc = self.nlp(text)\n",
    "            entities = [\n",
    "                ent.text.strip() for ent in doc.ents\n",
    "                if len(ent.text.strip()) >= 3 \n",
    "                and not ent.text.strip().isdigit()\n",
    "                and not re.fullmatch(r\"[#\\\\d/_.]+\", ent.text.strip())\n",
    "                and ent.label_ not in {\"DATE\", \"TIME\"}\n",
    "            ]\n",
    "            \n",
    "            # Add nodes and edges\n",
    "            for ent in entities:\n",
    "                if not self.G.has_node(ent):\n",
    "                    self.G.add_node(ent)\n",
    "            \n",
    "            # Add co-occurrence edges\n",
    "            for i, ent1 in enumerate(entities):\n",
    "                for ent2 in entities[i+1:]:\n",
    "                    if self.G.has_edge(ent1, ent2):\n",
    "                        self.G[ent1][ent2][\"weight\"] += 1\n",
    "                    else:\n",
    "                        self.G.add_edge(ent1, ent2, weight=1)\n",
    "\n",
    "    def process_for_analytics_only(self, df: pd.DataFrame):\n",
    "        \"\"\"Process dataframe for analytics without creating vector index\"\"\"\n",
    "        df = self._sanitize(df)\n",
    "        convs = self._df_to_conversations(df)\n",
    "        \n",
    "        # Just build entity graph for analytics\n",
    "        for conv in convs:\n",
    "            self.build_entity_graph([conv[\"text\"]])\n",
    "        \n",
    "        return convs\n",
    "\n",
    "    def spotify_wrapped(self, df: pd.DataFrame, user_id: str = None) -> Dict[str, Any]:\n",
    "        \"\"\"Generate Spotify-like wrapped statistics\"\"\"\n",
    "        df = self._sanitize(df)\n",
    "\n",
    "        num_chats = df.conversation_id.nunique()\n",
    "        num_messages = len(df)\n",
    "\n",
    "        user_df = df[df.author_role.eq(\"user\")]\n",
    "        assistant_df = df[df.author_role.eq(\"assistant\")]\n",
    "\n",
    "        # Entity extraction\n",
    "        ent_counter = collections.Counter()\n",
    "        for doc in self.nlp.pipe(user_df.body.tolist(), batch_size=128):\n",
    "            for ent in doc.ents:\n",
    "                txt = ent.text.strip()\n",
    "                if (len(txt) >= 3 and \n",
    "                    not txt.isdigit() and \n",
    "                    not re.fullmatch(r\"[#\\\\d/_.]+\", txt)):\n",
    "                    ent_counter[txt] += 1\n",
    "\n",
    "        top_entities = ent_counter.most_common(15)\n",
    "\n",
    "        # Top queries\n",
    "        queries = (\n",
    "            user_df.body\n",
    "            .map(lambda t: \" \".join(t.strip().split()[:8]).lower())\n",
    "            .value_counts()\n",
    "            .head(10)\n",
    "            .to_dict()\n",
    "        )\n",
    "\n",
    "        # Response tokens\n",
    "        response_tokens = int(assistant_df.body.str.split().map(len).sum())\n",
    "\n",
    "        # Programming languages\n",
    "        langs = [\"python\", \"javascript\", \"typescript\", \"c++\", \"c\", \"java\", \n",
    "                \"go\", \"rust\", \"ruby\", \"php\", \"scala\", \"sql\"]\n",
    "        lang_counts = {\n",
    "            lang: int(df.body.str.contains(fr\"\\b{re.escape(lang)}\\b\", case=False).sum())\n",
    "            for lang in langs\n",
    "        }\n",
    "        most_common_languages = {\n",
    "            k: v for k, v in sorted(lang_counts.items(), key=lambda p: p[1], reverse=True) \n",
    "            if v > 0\n",
    "        }\n",
    "\n",
    "        # Most active hour\n",
    "        most_active_hour = (\n",
    "            int(df.created_at.dt.hour.value_counts().idxmax()) \n",
    "            if not df.empty else None\n",
    "        )\n",
    "\n",
    "        return {\n",
    "            \"year\": datetime.utcnow().year,\n",
    "            \"user_id\": user_id or \"default\",\n",
    "            \"num_chats\": num_chats,\n",
    "            \"num_messages\": num_messages,\n",
    "            \"response_tokens\": response_tokens,\n",
    "            \"top_entities\": top_entities,\n",
    "            \"top_queries\": queries,\n",
    "            \"languages\": most_common_languages,\n",
    "            \"most_active_hour\": most_active_hour,\n",
    "            \"generated_at\": datetime.utcnow().isoformat(),\n",
    "        }\n",
    "\n",
    "    def graph_summary(self, top_k: int = 10) -> Dict[str, Any]:\n",
    "        \"\"\"Generate graph statistics\"\"\"\n",
    "        if self.G.number_of_nodes() == 0:\n",
    "            return {}\n",
    "        \n",
    "        pr = nx.pagerank(self.G, weight=\"weight\")\n",
    "        top_nodes = sorted(pr.items(), key=lambda x: x[1], reverse=True)[:top_k]\n",
    "        bridge_edges = sorted(\n",
    "            self.G.edges(data=True), \n",
    "            key=lambda e: e[2][\"weight\"], \n",
    "            reverse=True\n",
    "        )[:top_k]\n",
    "        \n",
    "        return {\n",
    "            \"pagerank_top\": top_nodes,\n",
    "            \"bridge_edges\": [(s, t, d[\"weight\"]) for s, t, d in bridge_edges],\n",
    "        }\n",
    "\n",
    "class ChatGraphBackend:\n",
    "    \"\"\"Vector search + Anthropic API wrapper\"\"\"\n",
    "    \n",
    "    def __init__(self, embedder: MiniChatEmbedder):\n",
    "        storage_ctx = StorageContext.from_defaults(vector_store=embedder.store)\n",
    "        self.vector_idx = VectorStoreIndex([], storage_context=storage_ctx)\n",
    "        self.entity_graph = embedder.G\n",
    "        self.anthropic_client = anthropic_client\n",
    "\n",
    "    def retrieve(self, query: str, k: int = 8) -> List[NodeWithScore]:\n",
    "        \"\"\"Retrieve k context nodes\"\"\"\n",
    "        retriever = self.vector_idx.as_retriever(similarity_top_k=k)\n",
    "        return retriever.retrieve(query)\n",
    "\n",
    "    def answer(self, query: str, k: int = 8) -> str:\n",
    "        \"\"\"Retrieve context and generate answer\"\"\"\n",
    "        nodes = self.retrieve(query, k=k)\n",
    "        context = \"\\n\\n\".join(n.node.text for n in nodes)\n",
    "\n",
    "        system_prompt = (\n",
    "            \"You are an expert assistant who answers using the provided context. \"\n",
    "            \"If the answer isn't in the context, say you don't know.\"\n",
    "        )\n",
    "        \n",
    "        prompt = f\"Context:\\n---\\n{context}\\n---\\n\\nQuestion: {query}\\nAnswer:\"\n",
    "        \n",
    "        response = self.anthropic_client.messages.create(\n",
    "            model=\"claude-3-haiku-20240307\",\n",
    "            max_tokens=300,\n",
    "            system=system_prompt,\n",
    "            messages=[{\"role\": \"user\", \"content\": prompt}]\n",
    "        )\n",
    "        \n",
    "        return response.content[0].text.strip()\n",
    "\n",
    "# ── Data loading functions ──────────────────────────────────────────\n",
    "\n",
    "def load_conversations(path: str | Path) -> List[Dict[str, Any]]:\n",
    "    \"\"\"Load conversations from JSON file\"\"\"\n",
    "    path = Path(path)\n",
    "    with path.open(\"r\", encoding=\"utf-8\") as f:\n",
    "        conversations = json.load(f)\n",
    "    if not isinstance(conversations, list):\n",
    "        raise ValueError(f\"Expected list, got {type(conversations).__name__}\")\n",
    "    return conversations\n",
    "\n",
    "def _to_text(part: Any) -> str:\n",
    "    \"\"\"Convert message part to text\"\"\"\n",
    "    if isinstance(part, str):\n",
    "        return part\n",
    "    if isinstance(part, dict) and \"text\" in part:\n",
    "        return part[\"text\"]\n",
    "    return json.dumps(part, ensure_ascii=False)\n",
    "\n",
    "def parse_chatgpt_json(conv: Dict[str, Any]) -> pd.DataFrame:\n",
    "    \"\"\"Parse ChatGPT conversation format\"\"\"\n",
    "    rows = []\n",
    "    conv_id = conv.get(\"conversation_id\")\n",
    "    title = conv.get(\"title\", \"\")\n",
    "\n",
    "    for node in conv.get(\"mapping\", {}).values():\n",
    "        msg = node.get(\"message\")\n",
    "        if msg is None:\n",
    "            continue\n",
    "\n",
    "        parts = msg.get(\"content\", {}).get(\"parts\", [])\n",
    "        body = \"\\n\".join(_to_text(p) for p in parts).strip()\n",
    "        if not body:\n",
    "            continue\n",
    "\n",
    "        rows.append({\n",
    "            \"conversation_id\": conv_id,\n",
    "            \"email\": \"ahilanks@gmail.com\",\n",
    "            \"title\": title,\n",
    "            \"body\": body,\n",
    "            \"embeddings_json\": None,\n",
    "            \"created_at\": datetime.utcfromtimestamp(msg[\"create_time\"]).isoformat(),\n",
    "            \"company\": \"gpt\",\n",
    "            \"author_role\": (msg.get(\"author\") or {}).get(\"role\"),\n",
    "        })\n",
    "\n",
    "    col_order = [\n",
    "        \"conversation_id\", \"email\", \"title\", \"body\",\n",
    "        \"embeddings_json\", \"created_at\", \"company\", \"author_role\",\n",
    "    ]\n",
    "    return pd.DataFrame(rows)[col_order]\n",
    "\n",
    "def conversations_to_dataframe(\n",
    "    path: str | Path,\n",
    "    drop_empty: bool = True,\n",
    "    drop_empty_convs: bool = True,\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"Convert conversations file to dataframe\"\"\"\n",
    "    convs = load_conversations(path)\n",
    "    dfs = [parse_chatgpt_json(c) for c in convs]\n",
    "    df = pd.concat(dfs, ignore_index=True)\n",
    "\n",
    "    if drop_empty:\n",
    "        df = df[df[\"body\"].str.strip().astype(bool)]\n",
    "\n",
    "    if drop_empty_convs:\n",
    "        df = df[df[\"conversation_id\"].isin(df[\"conversation_id\"].unique())]\n",
    "\n",
    "    # Clean up infinite values\n",
    "    df = df.replace([np.inf, -np.inf], np.nan)\n",
    "    clean_df = df.where(pd.notnull(df), None)\n",
    "\n",
    "    return clean_df.reset_index(drop=True)\n",
    "\n",
    "def batched_embed_and_insert(\n",
    "    df: pd.DataFrame,\n",
    "    table_name: str,\n",
    "    batch_size: int = 10,  # Smaller batches for embedding API\n",
    "    max_retries: int = 3,\n",
    "):\n",
    "    \"\"\"Generate embeddings and insert data in batches\"\"\"\n",
    "    embed_model = VoyageEmbedding(\n",
    "        model_name=MODEL_NAME,\n",
    "        voyage_api_key=VOYAGE_KEY\n",
    "    )\n",
    "    \n",
    "    tbl = client.table(table_name)\n",
    "    records = df.to_dict(orient=\"records\")\n",
    "    \n",
    "    print(f\"Processing {len(records)} records in batches of {batch_size}...\")\n",
    "\n",
    "    for start in range(0, len(records), batch_size):\n",
    "        chunk = records[start:start + batch_size]\n",
    "        \n",
    "        print(f\"Processing batch {start//batch_size + 1}/{(len(records)-1)//batch_size + 1} \"\n",
    "              f\"(rows {start}-{start+len(chunk)-1})\")\n",
    "\n",
    "        # Generate embeddings for this batch\n",
    "        texts_to_embed = [r[\"body\"] for r in chunk]\n",
    "        \n",
    "        try:\n",
    "            # Batch embed all texts at once (more efficient)\n",
    "            embeddings = embed_model.get_text_embedding_batch(texts_to_embed)\n",
    "            \n",
    "            # Add embeddings to records\n",
    "            for i, r in enumerate(chunk):\n",
    "                r[\"embeddings_json\"] = json.dumps({\"conversation\": embeddings[i]})\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"⚠️  Embedding batch failed, falling back to individual embeds: {e}\")\n",
    "            # Fallback to individual embeddings\n",
    "            for r in chunk:\n",
    "                try:\n",
    "                    conv_emb = embed_model.get_text_embedding(r[\"body\"])\n",
    "                    r[\"embeddings_json\"] = json.dumps({\"conversation\": conv_emb})\n",
    "                except Exception as embed_err:\n",
    "                    print(f\"⚠️  Failed to embed text: {embed_err}\")\n",
    "                    r[\"embeddings_json\"] = json.dumps({\"conversation\": [0.0] * DIMENSION})\n",
    "\n",
    "        # Insert batch into database with retry logic\n",
    "        attempt = 0\n",
    "        while True:\n",
    "            try:\n",
    "                resp = tbl.upsert(chunk).execute()\n",
    "                print(f\"✓ Inserted {len(chunk)} rows\")\n",
    "                break\n",
    "            except Exception as err:\n",
    "                attempt += 1\n",
    "                if attempt > max_retries:\n",
    "                    print(f\"❌ Failed to insert batch after {max_retries} retries: {err}\")\n",
    "                    raise\n",
    "                wait = 2 ** attempt\n",
    "                print(f\"⚠️  Insert failed ({err}); retry {attempt} in {wait}s\")\n",
    "                time.sleep(wait)\n",
    "        \n",
    "        # Small delay between batches to be nice to the APIs\n",
    "        time.sleep(0.5)\n",
    "\n",
    "# ── Main execution ──────────────────────────────────────────────────\n",
    "\n",
    "\n",
    "# Optional: Function to create vector index later if needed\n",
    "def create_vector_index_from_db():\n",
    "    \"\"\"Create vector index from existing database data (run this later if needed)\"\"\"\n",
    "    print(\"Creating vector index from existing database data...\")\n",
    "    embedder = MiniChatEmbedder()\n",
    "    \n",
    "    # The vector store will automatically load existing data\n",
    "    storage_ctx = StorageContext.from_defaults(vector_store=embedder.store)\n",
    "    idx = VectorStoreIndex([], storage_context=storage_ctx)\n",
    "    \n",
    "    print(\"✅ Vector index created and ready for queries!\")\n",
    "    return idx\n",
    "\n",
    "def update_embeddings_for_email(\n",
    "    email: str,\n",
    "    table_name: str = TABLE_NAME,\n",
    "    batch_size: int = 50,\n",
    "    max_retries: int = 3,\n",
    "):\n",
    "    \"\"\"\n",
    "    Pull rows for specific email, regenerate embeddings, and update database\n",
    "    \n",
    "    Args:\n",
    "        email: Email address to filter by\n",
    "        table_name: Database table name\n",
    "        batch_size: Number of rows to process per batch\n",
    "        max_retries: Max retry attempts for failed operations\n",
    "    \"\"\"\n",
    "    print(f\"🔄 Updating embeddings for email: {email}\")\n",
    "    \n",
    "    # Initialize embedding model\n",
    "    embed_model = VoyageEmbedding(\n",
    "        model_name=MODEL_NAME,\n",
    "        voyage_api_key=VOYAGE_KEY\n",
    "    )\n",
    "    \n",
    "    # Pull existing data for this email\n",
    "    print(\"📥 Fetching existing data from database...\")\n",
    "    try:\n",
    "        response = client.table(table_name).select(\"*\").eq(\"email\", email).execute()\n",
    "        \n",
    "        if not response.data:\n",
    "            print(f\"❌ No data found for email: {email}\")\n",
    "            return\n",
    "            \n",
    "        rows = response.data\n",
    "        print(f\"✅ Found {len(rows)} rows for {email}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Failed to fetch data: {e}\")\n",
    "        return\n",
    "    \n",
    "    # Process in batches\n",
    "    print(f\"🔄 Processing {len(rows)} rows in batches of {batch_size}...\")\n",
    "    updated_count = 0\n",
    "    \n",
    "    for start in range(0, len(rows), batch_size):\n",
    "        batch = rows[start:start + batch_size]\n",
    "        batch_num = start // batch_size + 1\n",
    "        total_batches = (len(rows) - 1) // batch_size + 1\n",
    "        \n",
    "        print(f\"\\n📦 Processing batch {batch_num}/{total_batches} \"\n",
    "              f\"(rows {start+1}-{start+len(batch)})\")\n",
    "        \n",
    "        # Extract texts and IDs for this batch\n",
    "        texts_to_embed = []\n",
    "        row_ids = []\n",
    "        \n",
    "        for row in batch:\n",
    "            if not row.get(\"body\"):\n",
    "                print(f\"⚠️  Skipping row with empty body (ID: {row.get('id', 'unknown')})\")\n",
    "                continue\n",
    "            texts_to_embed.append(row[\"body\"])\n",
    "            row_ids.append(row[\"id\"])  # Assuming there's an 'id' primary key\n",
    "        \n",
    "        if not texts_to_embed:\n",
    "            print(\"⚠️  No valid texts in this batch, skipping...\")\n",
    "            continue\n",
    "        \n",
    "        # Generate new embeddings\n",
    "        print(f\"🧠 Generating embeddings for {len(texts_to_embed)} texts...\")\n",
    "        try:\n",
    "            # Try batch embedding first (more efficient)\n",
    "            embeddings = embed_model.get_text_embedding_batch(texts_to_embed)\n",
    "            print(\"✅ Batch embedding successful\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"⚠️  Batch embedding failed ({e}), falling back to individual embeds...\")\n",
    "            embeddings = []\n",
    "            for i, text in enumerate(texts_to_embed):\n",
    "                try:\n",
    "                    emb = embed_model.get_text_embedding(text)\n",
    "                    embeddings.append(emb)\n",
    "                    if (i + 1) % 10 == 0:\n",
    "                        print(f\"   Generated {i+1}/{len(texts_to_embed)} embeddings...\")\n",
    "                except Exception as embed_err:\n",
    "                    print(f\"❌ Failed to embed text {i+1}: {embed_err}\")\n",
    "                    # Use zero vector as fallback\n",
    "                    embeddings.append([0.0] * DIMENSION)\n",
    "        \n",
    "        # Prepare updates\n",
    "        updates = []\n",
    "        for i, (row_id, embedding) in enumerate(zip(row_ids, embeddings)):\n",
    "            updates.append({\n",
    "                \"id\": row_id,\n",
    "                \"embeddings_json\": json.dumps({\"conversation\": embedding})\n",
    "            })\n",
    "        \n",
    "        # Update database with retry logic\n",
    "        print(f\"💾 Updating {len(updates)} rows in database...\")\n",
    "        attempt = 0\n",
    "        while True:\n",
    "            try:\n",
    "                # Use upsert to update existing rows\n",
    "                update_response = client.table(table_name).upsert(updates).execute()\n",
    "                \n",
    "                if update_response.data:\n",
    "                    batch_updated = len(update_response.data)\n",
    "                    updated_count += batch_updated\n",
    "                    print(f\"✅ Updated {batch_updated} rows\")\n",
    "                else:\n",
    "                    print(\"⚠️  Update returned no data (might still be successful)\")\n",
    "                    updated_count += len(updates)\n",
    "                \n",
    "                break\n",
    "                \n",
    "            except Exception as err:\n",
    "                attempt += 1\n",
    "                if attempt > max_retries:\n",
    "                    print(f\"❌ Failed to update batch after {max_retries} retries: {err}\")\n",
    "                    print(\"🛑 Stopping process to avoid data inconsistency\")\n",
    "                    return\n",
    "                    \n",
    "                wait = 2 ** attempt\n",
    "                print(f\"⚠️  Update failed ({err}); retry {attempt}/{max_retries} in {wait}s\")\n",
    "                time.sleep(wait)\n",
    "        \n",
    "        # Small delay between batches\n",
    "        time.sleep(0.5)\n",
    "    \n",
    "    print(f\"\\n🎉 Successfully updated embeddings for {updated_count} rows!\")\n",
    "    print(f\"📧 Email: {email}\")\n",
    "    print(f\"📊 Total processed: {len(rows)} rows\")\n",
    "    print(f\"✅ Total updated: {updated_count} rows\")\n",
    "\n",
    "def bulk_update_embeddings_by_emails(\n",
    "    emails: List[str], \n",
    "    table_name: str = TABLE_NAME,\n",
    "    batch_size: int = 50\n",
    "):\n",
    "    \"\"\"\n",
    "    Update embeddings for multiple emails\n",
    "    \n",
    "    Args:\n",
    "        emails: List of email addresses\n",
    "        table_name: Database table name  \n",
    "        batch_size: Batch size for processing\n",
    "    \"\"\"\n",
    "    print(f\"🔄 Bulk updating embeddings for {len(emails)} emails...\")\n",
    "    \n",
    "    for i, email in enumerate(emails, 1):\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"📧 Processing email {i}/{len(emails)}: {email}\")\n",
    "        print(f\"{'='*60}\")\n",
    "        \n",
    "        try:\n",
    "            update_embeddings_for_email(email, table_name, batch_size)\n",
    "        except Exception as e:\n",
    "            print(f\"❌ Failed to process {email}: {e}\")\n",
    "            continue\n",
    "    \n",
    "    print(f\"\\n🎉 Bulk update complete for {len(emails)} emails!\")\n",
    "\n",
    "# Helper function to check what emails exist in the database\n",
    "def list_emails_in_database(table_name: str = TABLE_NAME) -> List[str]:\n",
    "    \"\"\"Get list of unique emails in the database\"\"\"\n",
    "    try:\n",
    "        response = client.table(table_name).select(\"email\").execute()\n",
    "        emails = list(set(row[\"email\"] for row in response.data if row.get(\"email\")))\n",
    "        emails.sort()\n",
    "        return emails\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Failed to fetch emails: {e}\")\n",
    "        return []\n",
    "\n",
    "# Optional: Function to create vector index later if needed\n",
    "def create_vector_index_from_db():\n",
    "    \"\"\"Create vector index from existing database data (run this later if needed)\"\"\"\n",
    "    print(\"Creating vector index from existing database data...\")\n",
    "    embedder = MiniChatEmbedder()\n",
    "    \n",
    "    # The vector store will automatically load existing data\n",
    "    storage_ctx = StorageContext.from_defaults(vector_store=embedder.store)\n",
    "    idx = VectorStoreIndex([], storage_context=storage_ctx)\n",
    "    \n",
    "    print(\"✅ Vector index created and ready for queries!\")\n",
    "    return idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b970bed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading conversations...\n",
      "Loaded 12953 messages from 2084 conversations\n",
      "Initializing embedder...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "voyage-3-lite is not the latest model by Voyage AI. Please note that `model_name` will be a required argument in the future. We recommend setting it explicitly. Please see https://docs.voyageai.com/docs/embeddings for the latest models offered by Voyage AI.\n",
      "voyage-3-lite is not the latest model by Voyage AI. Please note that `model_name` will be a required argument in the future. We recommend setting it explicitly. Please see https://docs.voyageai.com/docs/embeddings for the latest models offered by Voyage AI.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating analytics...\n",
      "Generating embeddings and inserting into database...\n",
      "Processing 12953 records in batches of 50...\n",
      "Processing batch 1/260 (rows 0-49)\n",
      "✓ Inserted 50 rows\n",
      "Processing batch 2/260 (rows 50-99)\n",
      "✓ Inserted 50 rows\n",
      "Processing batch 3/260 (rows 100-149)\n",
      "✓ Inserted 50 rows\n",
      "Processing batch 4/260 (rows 150-199)\n",
      "✓ Inserted 50 rows\n",
      "Processing batch 5/260 (rows 200-249)\n",
      "✓ Inserted 50 rows\n",
      "Processing batch 6/260 (rows 250-299)\n",
      "✓ Inserted 50 rows\n",
      "Processing batch 7/260 (rows 300-349)\n",
      "✓ Inserted 50 rows\n",
      "Processing batch 8/260 (rows 350-399)\n",
      "⚠️  Embedding batch failed, falling back to individual embeds: Request to model 'voyage-3-lite' failed. The example at index 28 in your batch has too many tokens and does not fit into the model's context window of 32000 tokens. Please lower the number of tokens in the listed example(s) or use truncation.\n",
      "⚠️  Failed to embed text: Request to model 'voyage-3-lite' failed. The example at index 0 in your batch has too many tokens and does not fit into the model's context window of 32000 tokens. Please lower the number of tokens in the listed example(s) or use truncation.\n",
      "✓ Inserted 50 rows\n",
      "Processing batch 9/260 (rows 400-449)\n",
      "✓ Inserted 50 rows\n",
      "Processing batch 10/260 (rows 450-499)\n",
      "✓ Inserted 50 rows\n",
      "Processing batch 11/260 (rows 500-549)\n",
      "✓ Inserted 50 rows\n",
      "Processing batch 12/260 (rows 550-599)\n",
      "✓ Inserted 50 rows\n",
      "Processing batch 13/260 (rows 600-649)\n",
      "✓ Inserted 50 rows\n",
      "Processing batch 14/260 (rows 650-699)\n",
      "✓ Inserted 50 rows\n",
      "Processing batch 15/260 (rows 700-749)\n",
      "✓ Inserted 50 rows\n",
      "Processing batch 16/260 (rows 750-799)\n",
      "✓ Inserted 50 rows\n",
      "Processing batch 17/260 (rows 800-849)\n",
      "✓ Inserted 50 rows\n",
      "Processing batch 18/260 (rows 850-899)\n",
      "✓ Inserted 50 rows\n",
      "Processing batch 19/260 (rows 900-949)\n",
      "✓ Inserted 50 rows\n",
      "Processing batch 20/260 (rows 950-999)\n",
      "✓ Inserted 50 rows\n",
      "Processing batch 21/260 (rows 1000-1049)\n",
      "✓ Inserted 50 rows\n",
      "Processing batch 22/260 (rows 1050-1099)\n",
      "✓ Inserted 50 rows\n",
      "Processing batch 23/260 (rows 1100-1149)\n",
      "✓ Inserted 50 rows\n",
      "Processing batch 24/260 (rows 1150-1199)\n",
      "✓ Inserted 50 rows\n",
      "Processing batch 25/260 (rows 1200-1249)\n",
      "✓ Inserted 50 rows\n",
      "Processing batch 26/260 (rows 1250-1299)\n",
      "✓ Inserted 50 rows\n",
      "Processing batch 27/260 (rows 1300-1349)\n",
      "✓ Inserted 50 rows\n",
      "Processing batch 28/260 (rows 1350-1399)\n",
      "✓ Inserted 50 rows\n",
      "Processing batch 29/260 (rows 1400-1449)\n",
      "✓ Inserted 50 rows\n",
      "Processing batch 30/260 (rows 1450-1499)\n",
      "✓ Inserted 50 rows\n",
      "Processing batch 31/260 (rows 1500-1549)\n",
      "✓ Inserted 50 rows\n",
      "Processing batch 32/260 (rows 1550-1599)\n",
      "✓ Inserted 50 rows\n",
      "Processing batch 33/260 (rows 1600-1649)\n",
      "✓ Inserted 50 rows\n",
      "Processing batch 34/260 (rows 1650-1699)\n",
      "✓ Inserted 50 rows\n",
      "Processing batch 35/260 (rows 1700-1749)\n",
      "✓ Inserted 50 rows\n",
      "Processing batch 36/260 (rows 1750-1799)\n",
      "✓ Inserted 50 rows\n",
      "Processing batch 37/260 (rows 1800-1849)\n",
      "✓ Inserted 50 rows\n",
      "Processing batch 38/260 (rows 1850-1899)\n",
      "✓ Inserted 50 rows\n",
      "Processing batch 39/260 (rows 1900-1949)\n",
      "✓ Inserted 50 rows\n",
      "Processing batch 40/260 (rows 1950-1999)\n",
      "✓ Inserted 50 rows\n",
      "Processing batch 41/260 (rows 2000-2049)\n",
      "✓ Inserted 50 rows\n",
      "Processing batch 42/260 (rows 2050-2099)\n",
      "✓ Inserted 50 rows\n",
      "Processing batch 43/260 (rows 2100-2149)\n",
      "✓ Inserted 50 rows\n",
      "Processing batch 44/260 (rows 2150-2199)\n",
      "✓ Inserted 50 rows\n",
      "Processing batch 45/260 (rows 2200-2249)\n",
      "✓ Inserted 50 rows\n",
      "Processing batch 46/260 (rows 2250-2299)\n",
      "✓ Inserted 50 rows\n",
      "Processing batch 47/260 (rows 2300-2349)\n",
      "✓ Inserted 50 rows\n",
      "Processing batch 48/260 (rows 2350-2399)\n",
      "✓ Inserted 50 rows\n",
      "Processing batch 49/260 (rows 2400-2449)\n",
      "✓ Inserted 50 rows\n",
      "Processing batch 50/260 (rows 2450-2499)\n",
      "✓ Inserted 50 rows\n",
      "Processing batch 51/260 (rows 2500-2549)\n",
      "✓ Inserted 50 rows\n",
      "Processing batch 52/260 (rows 2550-2599)\n",
      "✓ Inserted 50 rows\n",
      "Processing batch 53/260 (rows 2600-2649)\n",
      "✓ Inserted 50 rows\n",
      "Processing batch 54/260 (rows 2650-2699)\n",
      "✓ Inserted 50 rows\n",
      "Processing batch 55/260 (rows 2700-2749)\n",
      "✓ Inserted 50 rows\n",
      "Processing batch 56/260 (rows 2750-2799)\n",
      "✓ Inserted 50 rows\n",
      "Processing batch 57/260 (rows 2800-2849)\n",
      "✓ Inserted 50 rows\n",
      "Processing batch 58/260 (rows 2850-2899)\n",
      "✓ Inserted 50 rows\n",
      "Processing batch 59/260 (rows 2900-2949)\n",
      "✓ Inserted 50 rows\n",
      "Processing batch 60/260 (rows 2950-2999)\n",
      "✓ Inserted 50 rows\n",
      "Processing batch 61/260 (rows 3000-3049)\n",
      "✓ Inserted 50 rows\n",
      "Processing batch 62/260 (rows 3050-3099)\n",
      "✓ Inserted 50 rows\n",
      "Processing batch 63/260 (rows 3100-3149)\n",
      "✓ Inserted 50 rows\n",
      "Processing batch 64/260 (rows 3150-3199)\n",
      "✓ Inserted 50 rows\n",
      "Processing batch 65/260 (rows 3200-3249)\n",
      "✓ Inserted 50 rows\n",
      "Processing batch 66/260 (rows 3250-3299)\n",
      "✓ Inserted 50 rows\n",
      "Processing batch 67/260 (rows 3300-3349)\n",
      "✓ Inserted 50 rows\n",
      "Processing batch 68/260 (rows 3350-3399)\n",
      "✓ Inserted 50 rows\n",
      "Processing batch 69/260 (rows 3400-3449)\n",
      "✓ Inserted 50 rows\n",
      "Processing batch 70/260 (rows 3450-3499)\n",
      "✓ Inserted 50 rows\n",
      "Processing batch 71/260 (rows 3500-3549)\n",
      "✓ Inserted 50 rows\n",
      "Processing batch 72/260 (rows 3550-3599)\n",
      "✓ Inserted 50 rows\n",
      "Processing batch 73/260 (rows 3600-3649)\n",
      "✓ Inserted 50 rows\n",
      "Processing batch 74/260 (rows 3650-3699)\n",
      "✓ Inserted 50 rows\n",
      "Processing batch 75/260 (rows 3700-3749)\n",
      "✓ Inserted 50 rows\n",
      "Processing batch 76/260 (rows 3750-3799)\n",
      "✓ Inserted 50 rows\n",
      "Processing batch 77/260 (rows 3800-3849)\n",
      "✓ Inserted 50 rows\n",
      "Processing batch 78/260 (rows 3850-3899)\n",
      "✓ Inserted 50 rows\n",
      "Processing batch 79/260 (rows 3900-3949)\n",
      "✓ Inserted 50 rows\n",
      "Processing batch 80/260 (rows 3950-3999)\n",
      "✓ Inserted 50 rows\n",
      "Processing batch 81/260 (rows 4000-4049)\n",
      "✓ Inserted 50 rows\n",
      "Processing batch 82/260 (rows 4050-4099)\n",
      "✓ Inserted 50 rows\n",
      "Processing batch 83/260 (rows 4100-4149)\n",
      "✓ Inserted 50 rows\n",
      "Processing batch 84/260 (rows 4150-4199)\n",
      "✓ Inserted 50 rows\n",
      "Processing batch 85/260 (rows 4200-4249)\n",
      "✓ Inserted 50 rows\n",
      "Processing batch 86/260 (rows 4250-4299)\n",
      "✓ Inserted 50 rows\n",
      "Processing batch 87/260 (rows 4300-4349)\n",
      "✓ Inserted 50 rows\n",
      "Processing batch 88/260 (rows 4350-4399)\n",
      "✓ Inserted 50 rows\n",
      "Processing batch 89/260 (rows 4400-4449)\n",
      "✓ Inserted 50 rows\n",
      "Processing batch 90/260 (rows 4450-4499)\n",
      "✓ Inserted 50 rows\n",
      "Processing batch 91/260 (rows 4500-4549)\n",
      "✓ Inserted 50 rows\n",
      "Processing batch 92/260 (rows 4550-4599)\n",
      "✓ Inserted 50 rows\n",
      "Processing batch 93/260 (rows 4600-4649)\n",
      "✓ Inserted 50 rows\n",
      "Processing batch 94/260 (rows 4650-4699)\n",
      "✓ Inserted 50 rows\n",
      "Processing batch 95/260 (rows 4700-4749)\n",
      "✓ Inserted 50 rows\n",
      "Processing batch 96/260 (rows 4750-4799)\n",
      "✓ Inserted 50 rows\n",
      "Processing batch 97/260 (rows 4800-4849)\n",
      "✓ Inserted 50 rows\n",
      "Processing batch 98/260 (rows 4850-4899)\n",
      "✓ Inserted 50 rows\n",
      "Processing batch 99/260 (rows 4900-4949)\n",
      "✓ Inserted 50 rows\n",
      "Processing batch 100/260 (rows 4950-4999)\n",
      "✓ Inserted 50 rows\n",
      "Processing batch 101/260 (rows 5000-5049)\n",
      "✓ Inserted 50 rows\n",
      "Processing batch 102/260 (rows 5050-5099)\n",
      "✓ Inserted 50 rows\n",
      "Processing batch 103/260 (rows 5100-5149)\n",
      "✓ Inserted 50 rows\n",
      "Processing batch 104/260 (rows 5150-5199)\n",
      "✓ Inserted 50 rows\n",
      "Processing batch 105/260 (rows 5200-5249)\n",
      "✓ Inserted 50 rows\n",
      "Processing batch 106/260 (rows 5250-5299)\n",
      "✓ Inserted 50 rows\n",
      "Processing batch 107/260 (rows 5300-5349)\n",
      "✓ Inserted 50 rows\n",
      "Processing batch 108/260 (rows 5350-5399)\n",
      "✓ Inserted 50 rows\n",
      "Processing batch 109/260 (rows 5400-5449)\n",
      "✓ Inserted 50 rows\n",
      "Processing batch 110/260 (rows 5450-5499)\n",
      "✓ Inserted 50 rows\n",
      "Processing batch 111/260 (rows 5500-5549)\n",
      "✓ Inserted 50 rows\n",
      "Processing batch 112/260 (rows 5550-5599)\n",
      "✓ Inserted 50 rows\n",
      "Processing batch 113/260 (rows 5600-5649)\n",
      "✓ Inserted 50 rows\n",
      "Processing batch 114/260 (rows 5650-5699)\n",
      "✓ Inserted 50 rows\n",
      "Processing batch 115/260 (rows 5700-5749)\n",
      "✓ Inserted 50 rows\n",
      "Processing batch 116/260 (rows 5750-5799)\n",
      "✓ Inserted 50 rows\n",
      "Processing batch 117/260 (rows 5800-5849)\n",
      "✓ Inserted 50 rows\n",
      "Processing batch 118/260 (rows 5850-5899)\n",
      "✓ Inserted 50 rows\n",
      "Processing batch 119/260 (rows 5900-5949)\n",
      "✓ Inserted 50 rows\n",
      "Processing batch 120/260 (rows 5950-5999)\n",
      "✓ Inserted 50 rows\n",
      "Processing batch 121/260 (rows 6000-6049)\n",
      "✓ Inserted 50 rows\n",
      "Processing batch 122/260 (rows 6050-6099)\n",
      "✓ Inserted 50 rows\n",
      "Processing batch 123/260 (rows 6100-6149)\n",
      "✓ Inserted 50 rows\n",
      "Processing batch 124/260 (rows 6150-6199)\n",
      "✓ Inserted 50 rows\n",
      "Processing batch 125/260 (rows 6200-6249)\n",
      "✓ Inserted 50 rows\n",
      "Processing batch 126/260 (rows 6250-6299)\n",
      "✓ Inserted 50 rows\n",
      "Processing batch 127/260 (rows 6300-6349)\n",
      "✓ Inserted 50 rows\n",
      "Processing batch 128/260 (rows 6350-6399)\n",
      "✓ Inserted 50 rows\n",
      "Processing batch 129/260 (rows 6400-6449)\n",
      "✓ Inserted 50 rows\n",
      "Processing batch 130/260 (rows 6450-6499)\n",
      "✓ Inserted 50 rows\n",
      "Processing batch 131/260 (rows 6500-6549)\n",
      "✓ Inserted 50 rows\n",
      "Processing batch 132/260 (rows 6550-6599)\n",
      "✓ Inserted 50 rows\n",
      "Processing batch 133/260 (rows 6600-6649)\n",
      "✓ Inserted 50 rows\n",
      "Processing batch 134/260 (rows 6650-6699)\n",
      "✓ Inserted 50 rows\n",
      "Processing batch 135/260 (rows 6700-6749)\n",
      "✓ Inserted 50 rows\n",
      "Processing batch 136/260 (rows 6750-6799)\n",
      "✓ Inserted 50 rows\n",
      "Processing batch 137/260 (rows 6800-6849)\n",
      "✓ Inserted 50 rows\n",
      "Processing batch 138/260 (rows 6850-6899)\n",
      "✓ Inserted 50 rows\n",
      "Processing batch 139/260 (rows 6900-6949)\n",
      "✓ Inserted 50 rows\n",
      "Processing batch 140/260 (rows 6950-6999)\n",
      "✓ Inserted 50 rows\n",
      "Processing batch 141/260 (rows 7000-7049)\n",
      "✓ Inserted 50 rows\n",
      "Processing batch 142/260 (rows 7050-7099)\n",
      "✓ Inserted 50 rows\n",
      "Processing batch 143/260 (rows 7100-7149)\n",
      "✓ Inserted 50 rows\n",
      "Processing batch 144/260 (rows 7150-7199)\n",
      "✓ Inserted 50 rows\n",
      "Processing batch 145/260 (rows 7200-7249)\n",
      "✓ Inserted 50 rows\n",
      "Processing batch 146/260 (rows 7250-7299)\n",
      "✓ Inserted 50 rows\n",
      "Processing batch 147/260 (rows 7300-7349)\n",
      "✓ Inserted 50 rows\n",
      "Processing batch 148/260 (rows 7350-7399)\n",
      "✓ Inserted 50 rows\n",
      "Processing batch 149/260 (rows 7400-7449)\n",
      "✓ Inserted 50 rows\n",
      "Processing batch 150/260 (rows 7450-7499)\n",
      "✓ Inserted 50 rows\n",
      "Processing batch 151/260 (rows 7500-7549)\n",
      "✓ Inserted 50 rows\n",
      "Processing batch 152/260 (rows 7550-7599)\n",
      "✓ Inserted 50 rows\n",
      "Processing batch 153/260 (rows 7600-7649)\n",
      "✓ Inserted 50 rows\n",
      "Processing batch 154/260 (rows 7650-7699)\n",
      "✓ Inserted 50 rows\n",
      "Processing batch 155/260 (rows 7700-7749)\n",
      "✓ Inserted 50 rows\n",
      "Processing batch 156/260 (rows 7750-7799)\n",
      "✓ Inserted 50 rows\n",
      "Processing batch 157/260 (rows 7800-7849)\n",
      "✓ Inserted 50 rows\n",
      "Processing batch 158/260 (rows 7850-7899)\n",
      "✓ Inserted 50 rows\n",
      "Processing batch 159/260 (rows 7900-7949)\n",
      "✓ Inserted 50 rows\n",
      "Processing batch 160/260 (rows 7950-7999)\n",
      "✓ Inserted 50 rows\n",
      "Processing batch 161/260 (rows 8000-8049)\n",
      "✓ Inserted 50 rows\n",
      "Processing batch 162/260 (rows 8050-8099)\n",
      "✓ Inserted 50 rows\n",
      "Processing batch 163/260 (rows 8100-8149)\n",
      "✓ Inserted 50 rows\n",
      "Processing batch 164/260 (rows 8150-8199)\n",
      "✓ Inserted 50 rows\n",
      "Processing batch 165/260 (rows 8200-8249)\n",
      "✓ Inserted 50 rows\n",
      "Processing batch 166/260 (rows 8250-8299)\n",
      "✓ Inserted 50 rows\n",
      "Processing batch 167/260 (rows 8300-8349)\n",
      "✓ Inserted 50 rows\n",
      "Processing batch 168/260 (rows 8350-8399)\n",
      "✓ Inserted 50 rows\n",
      "Processing batch 169/260 (rows 8400-8449)\n",
      "✓ Inserted 50 rows\n",
      "Processing batch 170/260 (rows 8450-8499)\n",
      "✓ Inserted 50 rows\n",
      "Processing batch 171/260 (rows 8500-8549)\n",
      "✓ Inserted 50 rows\n",
      "Processing batch 172/260 (rows 8550-8599)\n",
      "✓ Inserted 50 rows\n",
      "Processing batch 173/260 (rows 8600-8649)\n",
      "✓ Inserted 50 rows\n",
      "Processing batch 174/260 (rows 8650-8699)\n",
      "✓ Inserted 50 rows\n",
      "Processing batch 175/260 (rows 8700-8749)\n",
      "✓ Inserted 50 rows\n",
      "Processing batch 176/260 (rows 8750-8799)\n",
      "✓ Inserted 50 rows\n",
      "Processing batch 177/260 (rows 8800-8849)\n",
      "✓ Inserted 50 rows\n",
      "Processing batch 178/260 (rows 8850-8899)\n",
      "✓ Inserted 50 rows\n",
      "Processing batch 179/260 (rows 8900-8949)\n",
      "✓ Inserted 50 rows\n",
      "Processing batch 180/260 (rows 8950-8999)\n",
      "✓ Inserted 50 rows\n",
      "Processing batch 181/260 (rows 9000-9049)\n",
      "✓ Inserted 50 rows\n",
      "Processing batch 182/260 (rows 9050-9099)\n",
      "✓ Inserted 50 rows\n",
      "Processing batch 183/260 (rows 9100-9149)\n",
      "✓ Inserted 50 rows\n",
      "Processing batch 184/260 (rows 9150-9199)\n",
      "✓ Inserted 50 rows\n",
      "Processing batch 185/260 (rows 9200-9249)\n",
      "✓ Inserted 50 rows\n",
      "Processing batch 186/260 (rows 9250-9299)\n",
      "✓ Inserted 50 rows\n",
      "Processing batch 187/260 (rows 9300-9349)\n",
      "✓ Inserted 50 rows\n",
      "Processing batch 188/260 (rows 9350-9399)\n",
      "✓ Inserted 50 rows\n",
      "Processing batch 189/260 (rows 9400-9449)\n",
      "✓ Inserted 50 rows\n",
      "Processing batch 190/260 (rows 9450-9499)\n",
      "✓ Inserted 50 rows\n",
      "Processing batch 191/260 (rows 9500-9549)\n",
      "✓ Inserted 50 rows\n",
      "Processing batch 192/260 (rows 9550-9599)\n",
      "✓ Inserted 50 rows\n",
      "Processing batch 193/260 (rows 9600-9649)\n",
      "✓ Inserted 50 rows\n",
      "Processing batch 194/260 (rows 9650-9699)\n",
      "✓ Inserted 50 rows\n",
      "Processing batch 195/260 (rows 9700-9749)\n",
      "✓ Inserted 50 rows\n",
      "Processing batch 196/260 (rows 9750-9799)\n",
      "✓ Inserted 50 rows\n",
      "Processing batch 197/260 (rows 9800-9849)\n",
      "✓ Inserted 50 rows\n",
      "Processing batch 198/260 (rows 9850-9899)\n",
      "✓ Inserted 50 rows\n",
      "Processing batch 199/260 (rows 9900-9949)\n",
      "✓ Inserted 50 rows\n",
      "Processing batch 200/260 (rows 9950-9999)\n",
      "✓ Inserted 50 rows\n",
      "Processing batch 201/260 (rows 10000-10049)\n",
      "✓ Inserted 50 rows\n",
      "Processing batch 202/260 (rows 10050-10099)\n",
      "✓ Inserted 50 rows\n",
      "Processing batch 203/260 (rows 10100-10149)\n",
      "✓ Inserted 50 rows\n",
      "Processing batch 204/260 (rows 10150-10199)\n",
      "✓ Inserted 50 rows\n",
      "Processing batch 205/260 (rows 10200-10249)\n",
      "✓ Inserted 50 rows\n",
      "Processing batch 206/260 (rows 10250-10299)\n",
      "✓ Inserted 50 rows\n",
      "Processing batch 207/260 (rows 10300-10349)\n",
      "✓ Inserted 50 rows\n",
      "Processing batch 208/260 (rows 10350-10399)\n",
      "✓ Inserted 50 rows\n",
      "Processing batch 209/260 (rows 10400-10449)\n",
      "✓ Inserted 50 rows\n",
      "Processing batch 210/260 (rows 10450-10499)\n",
      "✓ Inserted 50 rows\n",
      "Processing batch 211/260 (rows 10500-10549)\n",
      "✓ Inserted 50 rows\n",
      "Processing batch 212/260 (rows 10550-10599)\n",
      "✓ Inserted 50 rows\n",
      "Processing batch 213/260 (rows 10600-10649)\n",
      "✓ Inserted 50 rows\n",
      "Processing batch 214/260 (rows 10650-10699)\n",
      "✓ Inserted 50 rows\n",
      "Processing batch 215/260 (rows 10700-10749)\n",
      "✓ Inserted 50 rows\n",
      "Processing batch 216/260 (rows 10750-10799)\n",
      "✓ Inserted 50 rows\n",
      "Processing batch 217/260 (rows 10800-10849)\n",
      "✓ Inserted 50 rows\n",
      "Processing batch 218/260 (rows 10850-10899)\n",
      "✓ Inserted 50 rows\n",
      "Processing batch 219/260 (rows 10900-10949)\n",
      "✓ Inserted 50 rows\n",
      "Processing batch 220/260 (rows 10950-10999)\n",
      "✓ Inserted 50 rows\n",
      "Processing batch 221/260 (rows 11000-11049)\n",
      "✓ Inserted 50 rows\n",
      "Processing batch 222/260 (rows 11050-11099)\n",
      "✓ Inserted 50 rows\n",
      "Processing batch 223/260 (rows 11100-11149)\n",
      "✓ Inserted 50 rows\n",
      "Processing batch 224/260 (rows 11150-11199)\n",
      "✓ Inserted 50 rows\n",
      "Processing batch 225/260 (rows 11200-11249)\n",
      "✓ Inserted 50 rows\n",
      "Processing batch 226/260 (rows 11250-11299)\n",
      "✓ Inserted 50 rows\n",
      "Processing batch 227/260 (rows 11300-11349)\n",
      "✓ Inserted 50 rows\n",
      "Processing batch 228/260 (rows 11350-11399)\n",
      "✓ Inserted 50 rows\n",
      "Processing batch 229/260 (rows 11400-11449)\n",
      "✓ Inserted 50 rows\n",
      "Processing batch 230/260 (rows 11450-11499)\n",
      "✓ Inserted 50 rows\n",
      "Processing batch 231/260 (rows 11500-11549)\n",
      "✓ Inserted 50 rows\n",
      "Processing batch 232/260 (rows 11550-11599)\n",
      "✓ Inserted 50 rows\n",
      "Processing batch 233/260 (rows 11600-11649)\n",
      "✓ Inserted 50 rows\n",
      "Processing batch 234/260 (rows 11650-11699)\n",
      "✓ Inserted 50 rows\n",
      "Processing batch 235/260 (rows 11700-11749)\n",
      "✓ Inserted 50 rows\n",
      "Processing batch 236/260 (rows 11750-11799)\n",
      "✓ Inserted 50 rows\n",
      "Processing batch 237/260 (rows 11800-11849)\n",
      "✓ Inserted 50 rows\n",
      "Processing batch 238/260 (rows 11850-11899)\n",
      "✓ Inserted 50 rows\n",
      "Processing batch 239/260 (rows 11900-11949)\n",
      "✓ Inserted 50 rows\n",
      "Processing batch 240/260 (rows 11950-11999)\n",
      "✓ Inserted 50 rows\n",
      "Processing batch 241/260 (rows 12000-12049)\n",
      "✓ Inserted 50 rows\n",
      "Processing batch 242/260 (rows 12050-12099)\n",
      "✓ Inserted 50 rows\n",
      "Processing batch 243/260 (rows 12100-12149)\n",
      "✓ Inserted 50 rows\n",
      "Processing batch 244/260 (rows 12150-12199)\n",
      "✓ Inserted 50 rows\n",
      "Processing batch 245/260 (rows 12200-12249)\n",
      "✓ Inserted 50 rows\n",
      "Processing batch 246/260 (rows 12250-12299)\n",
      "✓ Inserted 50 rows\n",
      "Processing batch 247/260 (rows 12300-12349)\n",
      "✓ Inserted 50 rows\n",
      "Processing batch 248/260 (rows 12350-12399)\n",
      "✓ Inserted 50 rows\n",
      "Processing batch 249/260 (rows 12400-12449)\n",
      "✓ Inserted 50 rows\n",
      "Processing batch 250/260 (rows 12450-12499)\n",
      "✓ Inserted 50 rows\n",
      "Processing batch 251/260 (rows 12500-12549)\n",
      "✓ Inserted 50 rows\n",
      "Processing batch 252/260 (rows 12550-12599)\n",
      "✓ Inserted 50 rows\n",
      "Processing batch 253/260 (rows 12600-12649)\n",
      "✓ Inserted 50 rows\n",
      "Processing batch 254/260 (rows 12650-12699)\n",
      "✓ Inserted 50 rows\n",
      "Processing batch 255/260 (rows 12700-12749)\n",
      "✓ Inserted 50 rows\n",
      "Processing batch 256/260 (rows 12750-12799)\n",
      "✓ Inserted 50 rows\n",
      "Processing batch 257/260 (rows 12800-12849)\n",
      "✓ Inserted 50 rows\n",
      "Processing batch 258/260 (rows 12850-12899)\n",
      "✓ Inserted 50 rows\n",
      "Processing batch 259/260 (rows 12900-12949)\n",
      "✓ Inserted 50 rows\n",
      "Processing batch 260/260 (rows 12950-12952)\n",
      "✓ Inserted 3 rows\n",
      "✅ Complete! Your data is now in Supabase with embeddings.\n",
      "Analytics summary:\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'wrapped' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 30\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m✅ Complete! Your data is now in Supabase with embeddings.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     29\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAnalytics summary:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 30\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m  - \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[43mwrapped\u001b[49m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnum_chats\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m conversations\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     31\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m  - \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mwrapped[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnum_messages\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m messages\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     32\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m  - \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mwrapped[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mresponse_tokens\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m response tokens\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'wrapped' is not defined"
     ]
    }
   ],
   "source": [
    "# Load and process data\n",
    "DATA_PATH = \"/Users/ahilankaruppusami/Downloads/a77aa29a280fad96d1324930986d583f2adc894294a4f10969f514af9748fcb0-2025-06-21-19-55-56-806d4e1bd1cb4a139efcafc6844cf74e/conversations.json\"  # Update this path\n",
    "print(\"Loading conversations...\")\n",
    "full_df = conversations_to_dataframe(DATA_PATH)\n",
    "print(f\"Loaded {len(full_df)} messages from {full_df.conversation_id.nunique()} conversations\")\n",
    "\n",
    "# Initialize embedder for analytics only (no vector index creation)\n",
    "print(\"Initializing embedder...\")\n",
    "embedder = MiniChatEmbedder()\n",
    "\n",
    "# print(\"Processing conversations for analytics...\")\n",
    "# convs = embedder.process_for_analytics_only(full_df)\n",
    "\n",
    "# Generate analytics\n",
    "print(\"Generating analytics...\")\n",
    "# graph = embedder.graph_summary()\n",
    "\n",
    "# print(\"Adding analytics to records...\")\n",
    "# full_df = full_df.copy()\n",
    "# full_df[\"wrapped_json\"] = json.dumps(wrapped)\n",
    "# full_df[\"graph_json\"] = json.dumps(graph)\n",
    "full_df[\"embeddings_json\"] = None \n",
    "\n",
    "print(\"Generating embeddings and inserting into database...\")\n",
    "batched_embed_and_insert(full_df, TABLE_NAME, batch_size=50)\n",
    "\n",
    "print(\"✅ Complete! Your data is now in Supabase with embeddings.\")\n",
    "# print(f\"Analytics summary:\")\n",
    "# print(f\"  - {wrapped['num_chats']} conversations\")\n",
    "# print(f\"  - {wrapped['num_messages']} messages\")\n",
    "# print(f\"  - {wrapped['response_tokens']} response tokens\")\n",
    "# print(f\"  - Top entities: {[e[0] for e in wrapped['top_entities'][:5]]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "aaae8c0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "voyage-3-lite is not the latest model by Voyage AI. Please note that `model_name` will be a required argument in the future. We recommend setting it explicitly. Please see https://docs.voyageai.com/docs/embeddings for the latest models offered by Voyage AI.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📧 Available emails in database:\n",
      "  - ahilanks101@gmail.com\n",
      "  - hello.devpatel@gmail.com\n",
      "\n",
      "🔄 Example: Updating embeddings for ahilanks101@gmail.com\n",
      "🔄 Updating embeddings for email: ahilanks101@gmail.com\n",
      "📥 Fetching existing data from database...\n",
      "✅ Found 2083 rows for ahilanks101@gmail.com\n",
      "🔄 Processing 2083 rows in batches of 50...\n",
      "\n",
      "📦 Processing batch 1/42 (rows 1-50)\n",
      "⚠️  Skipping row with empty body (ID: 43398)\n",
      "🧠 Generating embeddings for 49 texts...\n",
      "✅ Batch embedding successful\n",
      "💾 Updating 49 rows in database...\n",
      "✅ Updated 49 rows\n",
      "\n",
      "📦 Processing batch 2/42 (rows 51-100)\n",
      "🧠 Generating embeddings for 50 texts...\n",
      "✅ Batch embedding successful\n",
      "💾 Updating 50 rows in database...\n",
      "✅ Updated 50 rows\n",
      "\n",
      "📦 Processing batch 3/42 (rows 101-150)\n",
      "⚠️  Skipping row with empty body (ID: 43178)\n",
      "⚠️  Skipping row with empty body (ID: 43179)\n",
      "⚠️  Skipping row with empty body (ID: 43180)\n",
      "⚠️  Skipping row with empty body (ID: 43181)\n",
      "🧠 Generating embeddings for 46 texts...\n",
      "✅ Batch embedding successful\n",
      "💾 Updating 46 rows in database...\n",
      "✅ Updated 46 rows\n",
      "\n",
      "📦 Processing batch 4/42 (rows 151-200)\n",
      "⚠️  Skipping row with empty body (ID: 43145)\n",
      "⚠️  Skipping row with empty body (ID: 43176)\n",
      "⚠️  Skipping row with empty body (ID: 43177)\n",
      "🧠 Generating embeddings for 47 texts...\n",
      "✅ Batch embedding successful\n",
      "💾 Updating 47 rows in database...\n",
      "✅ Updated 47 rows\n",
      "\n",
      "📦 Processing batch 5/42 (rows 201-250)\n",
      "🧠 Generating embeddings for 50 texts...\n",
      "✅ Batch embedding successful\n",
      "💾 Updating 50 rows in database...\n",
      "✅ Updated 50 rows\n",
      "\n",
      "📦 Processing batch 6/42 (rows 251-300)\n",
      "🧠 Generating embeddings for 50 texts...\n",
      "✅ Batch embedding successful\n",
      "💾 Updating 50 rows in database...\n",
      "✅ Updated 50 rows\n",
      "\n",
      "📦 Processing batch 7/42 (rows 301-350)\n",
      "🧠 Generating embeddings for 50 texts...\n",
      "✅ Batch embedding successful\n",
      "💾 Updating 50 rows in database...\n",
      "✅ Updated 50 rows\n",
      "\n",
      "📦 Processing batch 8/42 (rows 351-400)\n",
      "🧠 Generating embeddings for 50 texts...\n",
      "✅ Batch embedding successful\n",
      "💾 Updating 50 rows in database...\n",
      "✅ Updated 50 rows\n",
      "\n",
      "📦 Processing batch 9/42 (rows 401-450)\n",
      "⚠️  Skipping row with empty body (ID: 43387)\n",
      "🧠 Generating embeddings for 49 texts...\n",
      "✅ Batch embedding successful\n",
      "💾 Updating 49 rows in database...\n",
      "✅ Updated 49 rows\n",
      "\n",
      "📦 Processing batch 10/42 (rows 451-500)\n",
      "⚠️  Skipping row with empty body (ID: 43435)\n",
      "⚠️  Skipping row with empty body (ID: 43480)\n",
      "🧠 Generating embeddings for 48 texts...\n",
      "✅ Batch embedding successful\n",
      "💾 Updating 48 rows in database...\n",
      "✅ Updated 48 rows\n",
      "\n",
      "📦 Processing batch 11/42 (rows 501-550)\n",
      "⚠️  Skipping row with empty body (ID: 43530)\n",
      "🧠 Generating embeddings for 49 texts...\n",
      "✅ Batch embedding successful\n",
      "💾 Updating 49 rows in database...\n",
      "✅ Updated 49 rows\n",
      "\n",
      "📦 Processing batch 12/42 (rows 551-600)\n",
      "🧠 Generating embeddings for 50 texts...\n",
      "✅ Batch embedding successful\n",
      "💾 Updating 50 rows in database...\n",
      "✅ Updated 50 rows\n",
      "\n",
      "📦 Processing batch 13/42 (rows 601-650)\n",
      "🧠 Generating embeddings for 50 texts...\n",
      "✅ Batch embedding successful\n",
      "💾 Updating 50 rows in database...\n",
      "✅ Updated 50 rows\n",
      "\n",
      "📦 Processing batch 14/42 (rows 651-700)\n",
      "⚠️  Skipping row with empty body (ID: 44398)\n",
      "⚠️  Skipping row with empty body (ID: 43679)\n",
      "🧠 Generating embeddings for 48 texts...\n",
      "✅ Batch embedding successful\n",
      "💾 Updating 48 rows in database...\n",
      "✅ Updated 48 rows\n",
      "\n",
      "📦 Processing batch 15/42 (rows 701-750)\n",
      "🧠 Generating embeddings for 50 texts...\n",
      "✅ Batch embedding successful\n",
      "💾 Updating 50 rows in database...\n",
      "✅ Updated 50 rows\n",
      "\n",
      "📦 Processing batch 16/42 (rows 751-800)\n",
      "🧠 Generating embeddings for 50 texts...\n",
      "✅ Batch embedding successful\n",
      "💾 Updating 50 rows in database...\n",
      "✅ Updated 50 rows\n",
      "\n",
      "📦 Processing batch 17/42 (rows 801-850)\n",
      "🧠 Generating embeddings for 50 texts...\n",
      "✅ Batch embedding successful\n",
      "💾 Updating 50 rows in database...\n",
      "✅ Updated 50 rows\n",
      "\n",
      "📦 Processing batch 18/42 (rows 851-900)\n",
      "🧠 Generating embeddings for 50 texts...\n",
      "✅ Batch embedding successful\n",
      "💾 Updating 50 rows in database...\n",
      "✅ Updated 50 rows\n",
      "\n",
      "📦 Processing batch 19/42 (rows 901-950)\n",
      "🧠 Generating embeddings for 50 texts...\n",
      "✅ Batch embedding successful\n",
      "💾 Updating 50 rows in database...\n",
      "✅ Updated 50 rows\n",
      "\n",
      "📦 Processing batch 20/42 (rows 951-1000)\n",
      "⚠️  Skipping row with empty body (ID: 43980)\n",
      "⚠️  Skipping row with empty body (ID: 43987)\n",
      "🧠 Generating embeddings for 48 texts...\n",
      "✅ Batch embedding successful\n",
      "💾 Updating 48 rows in database...\n",
      "✅ Updated 48 rows\n",
      "\n",
      "📦 Processing batch 21/42 (rows 1001-1050)\n",
      "⚠️  Skipping row with empty body (ID: 44013)\n",
      "⚠️  Skipping row with empty body (ID: 44015)\n",
      "🧠 Generating embeddings for 48 texts...\n",
      "✅ Batch embedding successful\n",
      "💾 Updating 48 rows in database...\n",
      "✅ Updated 48 rows\n",
      "\n",
      "📦 Processing batch 22/42 (rows 1051-1100)\n",
      "⚠️  Skipping row with empty body (ID: 44067)\n",
      "🧠 Generating embeddings for 49 texts...\n",
      "✅ Batch embedding successful\n",
      "💾 Updating 49 rows in database...\n",
      "✅ Updated 49 rows\n",
      "\n",
      "📦 Processing batch 23/42 (rows 1101-1150)\n",
      "⚠️  Skipping row with empty body (ID: 44110)\n",
      "🧠 Generating embeddings for 49 texts...\n",
      "✅ Batch embedding successful\n",
      "💾 Updating 49 rows in database...\n",
      "✅ Updated 49 rows\n",
      "\n",
      "📦 Processing batch 24/42 (rows 1151-1200)\n",
      "⚠️  Skipping row with empty body (ID: 44165)\n",
      "🧠 Generating embeddings for 49 texts...\n",
      "✅ Batch embedding successful\n",
      "💾 Updating 49 rows in database...\n",
      "✅ Updated 49 rows\n",
      "\n",
      "📦 Processing batch 25/42 (rows 1201-1250)\n",
      "⚠️  Skipping row with empty body (ID: 44203)\n",
      "🧠 Generating embeddings for 49 texts...\n",
      "✅ Batch embedding successful\n",
      "💾 Updating 49 rows in database...\n",
      "✅ Updated 49 rows\n",
      "\n",
      "📦 Processing batch 26/42 (rows 1251-1300)\n",
      "🧠 Generating embeddings for 50 texts...\n",
      "✅ Batch embedding successful\n",
      "💾 Updating 50 rows in database...\n",
      "✅ Updated 50 rows\n",
      "\n",
      "📦 Processing batch 27/42 (rows 1301-1350)\n",
      "🧠 Generating embeddings for 50 texts...\n",
      "✅ Batch embedding successful\n",
      "💾 Updating 50 rows in database...\n",
      "✅ Updated 50 rows\n",
      "\n",
      "📦 Processing batch 28/42 (rows 1351-1400)\n",
      "🧠 Generating embeddings for 50 texts...\n",
      "✅ Batch embedding successful\n",
      "💾 Updating 50 rows in database...\n",
      "✅ Updated 50 rows\n",
      "\n",
      "📦 Processing batch 29/42 (rows 1401-1450)\n",
      "⚠️  Skipping row with empty body (ID: 44455)\n",
      "🧠 Generating embeddings for 49 texts...\n",
      "✅ Batch embedding successful\n",
      "💾 Updating 49 rows in database...\n",
      "✅ Updated 49 rows\n",
      "\n",
      "📦 Processing batch 30/42 (rows 1451-1500)\n",
      "🧠 Generating embeddings for 50 texts...\n",
      "✅ Batch embedding successful\n",
      "💾 Updating 50 rows in database...\n",
      "✅ Updated 50 rows\n",
      "\n",
      "📦 Processing batch 31/42 (rows 1501-1550)\n",
      "🧠 Generating embeddings for 50 texts...\n",
      "✅ Batch embedding successful\n",
      "💾 Updating 50 rows in database...\n",
      "✅ Updated 50 rows\n",
      "\n",
      "📦 Processing batch 32/42 (rows 1551-1600)\n",
      "⚠️  Skipping row with empty body (ID: 44547)\n",
      "🧠 Generating embeddings for 49 texts...\n",
      "✅ Batch embedding successful\n",
      "💾 Updating 49 rows in database...\n",
      "✅ Updated 49 rows\n",
      "\n",
      "📦 Processing batch 33/42 (rows 1601-1650)\n",
      "⚠️  Skipping row with empty body (ID: 44590)\n",
      "⚠️  Skipping row with empty body (ID: 44597)\n",
      "⚠️  Skipping row with empty body (ID: 44615)\n",
      "🧠 Generating embeddings for 47 texts...\n",
      "✅ Batch embedding successful\n",
      "💾 Updating 47 rows in database...\n",
      "✅ Updated 47 rows\n",
      "\n",
      "📦 Processing batch 34/42 (rows 1651-1700)\n",
      "🧠 Generating embeddings for 50 texts...\n",
      "✅ Batch embedding successful\n",
      "💾 Updating 50 rows in database...\n",
      "✅ Updated 50 rows\n",
      "\n",
      "📦 Processing batch 35/42 (rows 1701-1750)\n",
      "🧠 Generating embeddings for 50 texts...\n",
      "✅ Batch embedding successful\n",
      "💾 Updating 50 rows in database...\n",
      "✅ Updated 50 rows\n",
      "\n",
      "📦 Processing batch 36/42 (rows 1751-1800)\n",
      "🧠 Generating embeddings for 50 texts...\n",
      "✅ Batch embedding successful\n",
      "💾 Updating 50 rows in database...\n",
      "✅ Updated 50 rows\n",
      "\n",
      "📦 Processing batch 37/42 (rows 1801-1850)\n",
      "⚠️  Skipping row with empty body (ID: 44817)\n",
      "⚠️  Skipping row with empty body (ID: 44822)\n",
      "🧠 Generating embeddings for 48 texts...\n",
      "✅ Batch embedding successful\n",
      "💾 Updating 48 rows in database...\n",
      "✅ Updated 48 rows\n",
      "\n",
      "📦 Processing batch 38/42 (rows 1851-1900)\n",
      "⚠️  Skipping row with empty body (ID: 44898)\n",
      "🧠 Generating embeddings for 49 texts...\n",
      "✅ Batch embedding successful\n",
      "💾 Updating 49 rows in database...\n",
      "✅ Updated 49 rows\n",
      "\n",
      "📦 Processing batch 39/42 (rows 1901-1950)\n",
      "⚠️  Skipping row with empty body (ID: 44913)\n",
      "🧠 Generating embeddings for 49 texts...\n",
      "✅ Batch embedding successful\n",
      "💾 Updating 49 rows in database...\n",
      "✅ Updated 49 rows\n",
      "\n",
      "📦 Processing batch 40/42 (rows 1951-2000)\n",
      "🧠 Generating embeddings for 50 texts...\n",
      "✅ Batch embedding successful\n",
      "💾 Updating 50 rows in database...\n",
      "✅ Updated 50 rows\n",
      "\n",
      "📦 Processing batch 41/42 (rows 2001-2050)\n",
      "🧠 Generating embeddings for 50 texts...\n",
      "✅ Batch embedding successful\n",
      "💾 Updating 50 rows in database...\n",
      "✅ Updated 50 rows\n",
      "\n",
      "📦 Processing batch 42/42 (rows 2051-2083)\n",
      "🧠 Generating embeddings for 33 texts...\n",
      "✅ Batch embedding successful\n",
      "💾 Updating 33 rows in database...\n",
      "✅ Updated 33 rows\n",
      "\n",
      "🎉 Successfully updated embeddings for 2052 rows!\n",
      "📧 Email: ahilanks101@gmail.com\n",
      "📊 Total processed: 2083 rows\n",
      "✅ Total updated: 2052 rows\n"
     ]
    }
   ],
   "source": [
    "print(\"📧 Available emails in database:\")\n",
    "emails = list_emails_in_database()\n",
    "for email in emails:\n",
    "    print(f\"  - {email}\")\n",
    "\n",
    "if emails:\n",
    "    print(f\"\\n🔄 Example: Updating embeddings for {emails[0]}\")\n",
    "    update_embeddings_for_email(\"ahilanks101@gmail.com\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9b8e39a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
